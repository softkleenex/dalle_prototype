{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vyuFJyZ0WrA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pip, importÍ¥ÄÎ¶¨, wandia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.') \n",
        "\n",
        "#python 3.9.23\n",
        "\n",
        "# üì¶ pip ÏµúÏã†Ìôî (optional)\n",
        "!{sys.executable} -m pip install pip==25.1.1\n",
        "\n",
        "# ‚úÖ PyTorch + torchvision (ÏÑ†ÌÉùÌïú Î≤ÑÏ†Ñ, CUDA ÏûêÎèô Í∞êÏßÄ or CPUÏö© Îî∞Î°ú Î™ÖÏãú Í∞ÄÎä•)\n",
        "!{sys.executable} -m pip install torch==2.1.2 torchvision==0.16.2\n",
        "\n",
        "# ü§ñ Transformers (ÏïàÏ†ï Î≤ÑÏ†Ñ)\n",
        "!{sys.executable} -m pip install transformers==4.53.1\n",
        "\n",
        "# üß© ÌïÑÏàò Ïú†Ìã∏\n",
        "!{sys.executable} -m pip install einops==0.8.1\n",
        "!{sys.executable} -m pip install regex\n",
        "!{sys.executable} -m pip install omegaconf==2.3.0\n",
        "!{sys.executable} -m pip install \"matplotlib>=3.5,<3.9\"\n",
        "!{sys.executable} -m pip install Pillow\n",
        "!{sys.executable} -m pip install numpy\n",
        "\n",
        "# üî§ BPE Tokenizer\n",
        "!{sys.executable} -m pip install cython\n",
        "!{sys.executable} -m pip install youtokentome\n",
        "\n",
        "# üß† OpenAI DALL¬∑E Í≥µÏãù VAE (.pkl Í∏∞Î∞ò)\n",
        "!{sys.executable} -m pip install git+https://github.com/openai/DALL-E\n",
        "\n",
        "\n",
        "# üîÅ numpy Ïû¨ÏÑ§Ïπò Î∞è Î≤ÑÏ†Ñ Îã§Ïö¥Í∑∏Î†àÏù¥Îìú (Í∂åÏû• ÏïàÏ†ï Î≤ÑÏ†Ñ)\n",
        "!{sys.executable} -m pip install --force-reinstall numpy==1.24.4\n",
        "\n",
        "!{sys.executable} -m pip install dalle-pytorch taming-transformers-rom1504  \n",
        "\n",
        "!{sys.executable} -m pip install git+https://github.com/openai/CLIP.git           \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Í∏∞Î≥∏ Ïú†Ìã∏\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from glob import glob\n",
        "import pkg_resources\n",
        "\n",
        "# PyTorch & Ìà¥ÌÇ∑\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Ïù¥ÎØ∏ÏßÄ Î∞è ÏãúÍ∞ÅÌôî\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tokenizer\n",
        "import youtokentome as yttm\n",
        "\n",
        "# Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò (Ìïú Î≤àÎßå Ïã§Ìñâ)\n",
        "!git clone https://github.com/lucidrains/DALLE-pytorch.git\n",
        "!pip install -e ./DALLE-pytorch\n",
        "!pip install taming-transformers-rom1504 einops omegaconf regex matplotlib Pillow numpy youtokentome\n",
        "\n",
        "# Í≤ΩÎ°ú Ï∂îÍ∞Ä\n",
        "sys.path.append('./DALLE-pytorch')\n",
        "\n",
        "# DALL¬∑E import\n",
        "from dalle_pytorch import DALLE, VQGanVAE\n",
        "\n",
        "# torch Ìï®ÏàòÎì§\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from math import sqrt\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from youtokentome import BPE, OutputType \n",
        "\n",
        "import wandb\n",
        "wandb.login(key=\"9f75da6a26ca1502b0b9429b23f6f3f0feef990d\")\n",
        "\n",
        "from youtokentome import OutputType\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import youtokentome as yttm\n",
        "from dalle_pytorch.vae import VQGanVAE as BaseVQGanVAE\n",
        "\n",
        "import clip\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ÎπÑÍµê Îç∞Ïù¥ÌÑ∞ Î°úÎìú"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_prompts_with_reference_images(images_root='images', image_exts=('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "    data = []\n",
        "\n",
        "    for folder_name in os.listdir(images_root):\n",
        "        folder_path = os.path.join(images_root, folder_name)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        prompt_file = os.path.join(folder_path, 'prompts.txt')\n",
        "        if not os.path.exists(prompt_file):\n",
        "            continue\n",
        "\n",
        "        # Ïù¥ÎØ∏ÏßÄ ÌõÑÎ≥¥ Î¶¨Ïä§Ìä∏\n",
        "        image_files = [\n",
        "            os.path.join(folder_path, fname)\n",
        "            for fname in os.listdir(folder_path)\n",
        "            if fname.lower().endswith(image_exts)\n",
        "        ]\n",
        "\n",
        "        if not image_files:\n",
        "            continue  # Ïù¥ÎØ∏ÏßÄ ÏóÜÏúºÎ©¥ skip\n",
        "\n",
        "        with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                prompt = line.strip()\n",
        "                if not prompt:\n",
        "                    continue\n",
        "\n",
        "                # Ïù¥ÎØ∏ÏßÄ Ï§ë ÌïòÎÇò ÎûúÎç§ ÏÑ†ÌÉù\n",
        "                ref_image_path = random.choice(image_files)\n",
        "                data.append({\n",
        "                    'prompt': prompt,\n",
        "                    'ref_image': ref_image_path,\n",
        "                    'folder': folder_path\n",
        "                })\n",
        "\n",
        "    return data\n",
        "\n",
        "# ‚úÖ ÏÇ¨Ïö© ÏòàÏãú\n",
        "pairs = load_prompts_with_reference_images()\n",
        "print(f\"{len(pairs)}Í∞úÏùò ÌîÑÎ°¨ÌîÑÌä∏-Ïù¥ÎØ∏ÏßÄ ÏåçÏù¥ Î°úÎìúÎêòÏóàÏäµÎãàÎã§.\")\n",
        "print(pairs[0])  # {'prompt': ..., 'ref_image': ..., 'folder': ...}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7XIByl3J8fB"
      },
      "source": [
        "### VAE ÌÅ¥ÎûòÏä§ Ï†ïÏùò Î∞è ÏÑ§Ïπò"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class VQGanVAE(BaseVQGanVAE):\n",
        "    def __init__(self, vqgan_model_path=None, vqgan_config_path=None, from_pretrained=True):\n",
        "        super().__init__()\n",
        "        if from_pretrained:\n",
        "            self.load_ckpt_and_config(vqgan_model_path, vqgan_config_path)\n",
        "        else:\n",
        "            self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        pass  # ÎÇòÏ§ëÏóê .ptÎßå Î∂àÎü¨Ïì∏ Í≤ΩÏö∞ ÌïÑÏöîÌïú Íµ¨Ï°∞ Ï†ïÏùò\n",
        "\n",
        "    def load_ckpt_and_config(self, model_path, config_path):\n",
        "        # ÏõêÎûò dalle_pytorchÏóê ÏûàÎäî VQGanVAE.load_ckpt_and_config Î©îÏÑúÎìú Ï∞∏Í≥†\n",
        "        import yaml\n",
        "        from omegaconf import OmegaConf\n",
        "        from taming.models.vqgan import VQModel\n",
        "        from taming.models import cond_transformer, vqgan\n",
        "\n",
        "        # config Î∂àÎü¨Ïò§Í∏∞\n",
        "        config = OmegaConf.load(config_path)\n",
        "        model = VQModel(**config.model.params)\n",
        "\n",
        "        # ckpt Î°úÎìú\n",
        "        sd = torch.load(model_path, map_location='cpu')['state_dict']\n",
        "        model.load_state_dict(sd, strict=False)\n",
        "\n",
        "        # ÏÜçÏÑ± Î∞îÏù∏Îî©\n",
        "        self.model = model\n",
        "        self.image_size = 256\n",
        "        self.num_tokens = 1024\n",
        "        self.num_layers = 4\n",
        "        \n",
        "        \n",
        "def get_codebook_indices(self, img):\n",
        "    z = self.model.encoder(img)\n",
        "    z = self.model.quant_conv(z)\n",
        "    quant, indices, _ = self.model.quantize(z)   # ÏùºÎ∞òÏ†ÅÏúºÎ°ú [B, H, W]\n",
        "\n",
        "    # üîí indices shape Ï†êÍ≤Ä Î∞è Î≥¥Ï†ï\n",
        "    if indices.ndim == 3:\n",
        "        B, H, W = indices.shape\n",
        "        indices = indices.view(B, -1)   # [B, H*W]\n",
        "    elif indices.ndim == 1:\n",
        "        indices = indices.unsqueeze(0)  # [1, N]\n",
        "    elif indices.ndim == 0:\n",
        "        indices = indices.view(1, 1)    # scalar ‚Üí [1, 1]\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected indices shape: {indices.shape}\")\n",
        "\n",
        "    return indices  # ‚úÖ Ìï≠ÏÉÅ [B, N] ÌòïÌÉú Î≥¥Ïû•\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dalle_pytorch.vae import VQGanVAE  # ÎèôÏùºÌïú Î™®ÎìàÏóêÏÑú Î°úÎìúÎêòÏñ¥Ïïº Ìï®\n",
        "def load_vae(fast=True, force_yaml=False, debug=True):\n",
        "\n",
        "    # ‚úÖ VAE Î°úÎìú\n",
        "    if fast and not force_yaml and os.path.exists(\"vqgan_vae.pt\"):\n",
        "        vae = VQGanVAE(\n",
        "            vqgan_model_path = \"vqgan_imagenet_f16_1024/ckpts/last.ckpt\",\n",
        "            vqgan_config_path = \"vqgan_imagenet_f16_1024/configs/model.yaml\"\n",
        "        )\n",
        "        vae.load_state_dict(torch.load(\"vqgan_vae.pt\"))\n",
        "    else:\n",
        "        vae = VQGanVAE(\n",
        "            vqgan_model_path = \"vqgan_imagenet_f16_1024/ckpts/last.ckpt\",\n",
        "            vqgan_config_path = \"vqgan_imagenet_f16_1024/configs/model.yaml\"\n",
        "        )\n",
        "        torch.save(vae.state_dict(), \"vqgan_vae.pt\")\n",
        "\n",
        "    vae.eval()\n",
        "#vae ÏΩîÎìúÎ∂Å ÏÇ¨Ïù¥Ï¶à 1024 ÎÖºÎ¨∏ÏóêÏÑúÎäî 8192\n",
        "\n",
        "\n",
        "    # ‚úÖ ÎîîÎ≤ÑÍ∑∏: Ïù¥ÎØ∏ÏßÄ + ÌÖçÏä§Ìä∏ ‚Üí concat ÌôïÏù∏\n",
        "    if debug:\n",
        "        # sample Ïù¥ÎØ∏ÏßÄ Ï§ÄÎπÑ\n",
        "        sample_path = \"sample.png\"\n",
        "        if not os.path.exists(sample_path):\n",
        "            print(f\"‚ö†Ô∏è ÏÉòÌîå Ïù¥ÎØ∏ÏßÄ ÏóÜÏùå: {sample_path}\")\n",
        "            return vae\n",
        "\n",
        "        # Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        image = Image.open(sample_path).convert(\"RGB\")\n",
        "        image_tensor = transform(image).unsqueeze(0)  # [1, 3, 256, 256]\n",
        "\n",
        "        # ‚úÖ Ïù¥ÎØ∏ÏßÄ ‚Üí ÌÜ†ÌÅ∞\n",
        "        with torch.no_grad():\n",
        "            image_tokens = vae.get_codebook_indices(image_tensor)  # [1, 1024] or [1024] or scalar Îì± Îã§ÏñëÌï®\n",
        "\n",
        "        # ÏïàÏ†ÑÌïòÍ≤å shape Ï∂úÎ†•\n",
        "        image_tokens = image_tokens.view(1, -1)  # Í∞ïÏ†úÎ°ú [1, N] ÌòïÌÉúÎ°ú Î∞îÍøîÏ§å\n",
        "        print(\"üß† VAE image token shape:\", image_tokens.shape)\n",
        "        print(\"    ÏòàÏãú ÌÜ†ÌÅ∞ ID:\", image_tokens[0][:10].tolist())\n",
        "\n",
        "        # ‚úÖ ÌÖçÏä§Ìä∏ ‚Üí ÌÜ†ÌÅ∞\n",
        "        bpe = yttm.BPE(model=\"bpe.model\")\n",
        "        prompt = \"a red square\"\n",
        "        text_ids = bpe.encode([prompt], output_type=yttm.OutputType.ID)[0]\n",
        "        text_ids = text_ids[:128] + [0] * (128 - len(text_ids))  # Ìå®Îî©\n",
        "        text_tensor = torch.tensor(text_ids).unsqueeze(0)  # [1, 128]\n",
        "\n",
        "        print(\"‚úèÔ∏è Text token shape:\", text_tensor.shape)  # [1, 128]\n",
        "\n",
        "        # ‚úÖ concat\n",
        "        full_input = torch.cat([text_tensor, image_tokens], dim=1)  # [1, 128 + 1024]\n",
        "        print(\"üì¶ ConcatÎêú transformer ÏûÖÎ†• shape:\", full_input.shape)  # [1, 1152]\n",
        "\n",
        "    return vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COCOÎç∞Ïù¥ÌÑ∞ÏÖã Ï≤òÎ¶¨ÌïòÍ∏∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#COCO Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ DALL¬∑E ÌïôÏäµÏóê Ïì∏ Ïàò ÏûàÎèÑÎ°ù Ï†ÑÏ≤òÎ¶¨\n",
        "\n",
        "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "IMG_DIR = \"train2014\"\n",
        "CAPTION_FILE = \"annotations/captions_train2014.json\"\n",
        "OUTPUT_DIR = \"coco_text_pairs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Ï∫°ÏÖò JSON Î°úÎìú\n",
        "with open(CAPTION_FILE, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# image_id ‚Üí caption Îß§Ìïë (Ï≤´ Ï∫°ÏÖòÎßå ÏÇ¨Ïö©)\n",
        "id_to_caption = {}\n",
        "for ann in data['annotations']:\n",
        "    img_id = ann['image_id']\n",
        "    caption = ann['caption']\n",
        "    if img_id not in id_to_caption:\n",
        "        id_to_caption[img_id] = caption\n",
        "\n",
        "# Î≥ÄÌôò Î£®ÌîÑ\n",
        "skipped, converted = 0, 0\n",
        "\n",
        "for img in tqdm(data['images'], desc=\"Converting COCO\"):\n",
        "    img_id = img['id']\n",
        "    filename = img['file_name']\n",
        "    if img_id not in id_to_caption:\n",
        "        continue\n",
        "\n",
        "    src_img = os.path.join(IMG_DIR, filename)\n",
        "    base = f\"{img_id:012d}\"\n",
        "    dst_img = os.path.join(OUTPUT_DIR, f\"{base}.jpg\")\n",
        "    dst_txt = os.path.join(OUTPUT_DIR, f\"{base}.txt\")\n",
        "\n",
        "    # ‚úÖ Ïù¥ÎØ∏ Î≥ÄÌôòÎêú Í≤ΩÏö∞ Í±¥ÎÑàÎõ∞Í∏∞\n",
        "    if os.path.exists(dst_img) and os.path.exists(dst_txt):\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    if os.path.exists(src_img):\n",
        "        shutil.copy(src_img, dst_img)\n",
        "        with open(dst_txt, \"w\") as f:\n",
        "            f.write(id_to_caption[img_id])\n",
        "        converted += 1\n",
        "\n",
        "# Í≤∞Í≥º ÏöîÏïΩ Ï∂úÎ†•\n",
        "print(f\"‚úÖ Î≥ÄÌôò ÏôÑÎ£å: {converted}Í∞ú ÏÉùÏÑ±, {skipped}Í∞úÎäî Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï¥ÏÑú Í±¥ÎÑàÎúÄ.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"captions.txt\"):\n",
        "    with open(\"captions.txt\", \"w\") as out:\n",
        "        for txt_file in glob(\"coco_text_pairs/*.txt\"):\n",
        "            with open(txt_file, \"r\") as f:\n",
        "                line = f.read().strip()\n",
        "                out.write(line + \"\\n\")\n",
        "    print(\"‚úÖ captions.txt ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "else:\n",
        "    print(\"‚úÖ captions.txt Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï® ‚Äî Í±¥ÎÑàÎúÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### bpe Î™®Îç∏(caption(text) 2 token) ÎßåÎì§Í∏∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#bpeÎ™®Îç∏ < Î¨∏Ïû•Ïùò Î∂ÑÌï† Î™®Îç∏\n",
        "#ÎÖºÎ¨∏ BPE Î™®Îç∏ Ïû¨ÌòÑ, \n",
        "yttm.BPE.train(\n",
        "    data='captions.txt',         # Ï†ÑÏ≤¥ Ï∫°ÏÖò ÌÖçÏä§Ìä∏ ÌååÏùº\n",
        "    model='bpe.model',\n",
        "    vocab_size=16384\n",
        ")\n",
        "#bpe.modelÏôÑÏÑ±\n",
        "\n",
        "bpe = yttm.BPE(model=\"bpe.model\")\n",
        "#ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∂àÎü¨Ïò®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CocoTextPairDataset(Dataset):\n",
        "    \n",
        "    \n",
        "    def __init__(self, folder, tokenizer=None, transform=None, text_seq_len=128, dropout_prob=0.1):#ÎÖºÎ¨∏ÏóêÏÑúÏùò bpe 10% drop Ïû¨ÌòÑ\n",
        "        self.folder = folder\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_seq_len = text_seq_len\n",
        "        self.transform = transform\n",
        "        self.dropout_prob = dropout_prob  # ‚¨ÖÔ∏è Ï∂îÍ∞Ä\n",
        "        self.img_paths = []\n",
        "\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if file.endswith('.jpg') and 'images' not in os.path.join(root, file):\n",
        "                    self.img_paths.append(os.path.join(root, file))\n",
        "        self.img_paths.sort()\n",
        "  \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        base_name = os.path.splitext(img_path)[0]\n",
        "        txt_path = f\"{base_name}.txt\"\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        with open(txt_path, 'r') as f:\n",
        "            caption = f.read().strip()\n",
        "\n",
        "        if self.tokenizer:\n",
        "            token_ids = self.tokenizer.encode(\n",
        "                [caption],\n",
        "                output_type=yttm.OutputType.ID,\n",
        "                dropout_prob=self.dropout_prob  # üéØ ÎÖºÎ¨∏ BPE dropout\n",
        "            )[0]\n",
        "            \n",
        "            # üîß Í∏∏Ïù¥ Í≥†Ï†ï + Ìå®Îî©\n",
        "            token_ids = token_ids[:self.text_seq_len]\n",
        "            token_ids += [0] * (self.text_seq_len - len(token_ids))\n",
        "            \n",
        "            text = torch.tensor(token_ids)\n",
        "        else:\n",
        "            text = None\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#ÎÖºÎ¨∏ Ïù¥ÎØ∏ÏßÄ Ï¶ùÍ∞ï Ïû¨ÌòÑ\n",
        "class DalleImageAugmentation:\n",
        "    def __init__(self, target_res=256):\n",
        "        self.target_res = target_res\n",
        "\n",
        "    def __call__(self, img):  # img: PIL.Image\n",
        "        from torchvision.transforms import functional as TF\n",
        "        import random\n",
        "\n",
        "        w, h = img.size\n",
        "        s_min = min(w, h)\n",
        "\n",
        "        # Random crop offset\n",
        "        off_h = random.randint(3 * (h - s_min) // 8, max(5 * (h - s_min) // 8, 3 * (h - s_min) // 8 + 1))\n",
        "        off_w = random.randint(3 * (w - s_min) // 8, max(5 * (w - s_min) // 8, 3 * (w - s_min) // 8 + 1))\n",
        "\n",
        "        # Square crop\n",
        "        img = TF.crop(img, top=off_h, left=off_w, height=s_min, width=s_min)\n",
        "\n",
        "        # Resize to random size between [target_res, 9/8 * target_res]\n",
        "        t_max = min(s_min, round(9 / 8 * self.target_res))\n",
        "        t = random.randint(self.target_res, t_max)\n",
        "        img = TF.resize(img, [t, t], interpolation=TF.InterpolationMode.BILINEAR)\n",
        "\n",
        "        # Final random crop to target size\n",
        "        img = TF.center_crop(img, [self.target_res, self.target_res])\n",
        "        img = TF.to_tensor(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe = yttm.BPE(model=\"bpe.model\")\n",
        "\n",
        "\n",
        "transform = DalleImageAugmentation(target_res=256)\n",
        "\n",
        "\n",
        "# ÌïôÏäµÏö©\n",
        "train_dataset = CocoTextPairDataset(\n",
        "    folder=\"coco_text_pairs\",\n",
        "    tokenizer=bpe,\n",
        "    transform=transform,\n",
        "    text_seq_len=128,\n",
        "    dropout_prob=0.1  # üéØ Ïù¥Í≤å ÌïµÏã¨\n",
        ")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "# ‚úÖ Ï∂îÎ°†/Í≤ÄÏ¶ùÏö© (BPE dropout ÏóÜÏùå)\n",
        "eval_dataset = CocoTextPairDataset(\n",
        "    folder=\"coco_text_pairs\",\n",
        "    tokenizer=bpe,\n",
        "    transform=transform,\n",
        "    text_seq_len=128,\n",
        "    dropout_prob=0.0\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import youtokentome as yttm\n",
        "\n",
        "print(bpe.vocab_size())  # => 16384Í∞Ä ÎÇòÏôÄÏïº ÎÖºÎ¨∏Í≥º ÎèôÏùº\n",
        "\n",
        "\n",
        "bpe = yttm.BPE(model='bpe.model')\n",
        "print(\"üìè BPE vocab size:\", bpe.vocab_size())\n",
        "\n",
        "vae = load_vae()\n",
        "print(\"üé® Codebook size:\", vae.codebook.n_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ìä∏ÎûúÏä§Ìè¨Î®∏ ÌïôÏäµ ÏãúÏûë"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# üí° VAEÎäî freeze (ÎØ∏Î¶¨ ÌïôÏäµÎêú Í≤É ÏÇ¨Ïö©)\n",
        "vae.eval()\n",
        "for param in vae.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# üíæ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû• Í≤ΩÎ°ú\n",
        "save_dir = \"checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ‚öôÔ∏è ÌïôÏäµ ÏÑ§Ï†ï\n",
        "dalle.train()\n",
        "optimizer = torch.optim.Adam(dalle.parameters(), lr=1e-4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ï°∞Í∏àÎßå ÌïôÏäµÌï¥Î≥¥Ïûê!\n",
        "num_epochs = 1\n",
        "max_batches = 250\n",
        "batch_size = 4\n",
        "used_captions = set()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    start_time = datetime.now()  # ‚è±Ô∏è ETA Í≥ÑÏÇ∞Ïö© ÏãúÏûë ÏãúÍ∞Å\n",
        "    pbar = tqdm(enumerate(train_dataloader), total= max_batches, desc = f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for step, (images, tokenized_texts) in pbar:\n",
        "        \n",
        "        if step >= max_batches:  # üîí Ï†úÌïúÎêú stepÍπåÏßÄÎßå ÌïôÏäµ\n",
        "            break\n",
        "\n",
        "        # GPU ÎòêÎäî MPS Ìï†Îãπ\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            image_tokens = vae.get_codebook_indices(images)  # ‚úÖ shape: [B, 1024]\n",
        "\n",
        "        loss = dalle(\n",
        "            text=tokenized_texts,\n",
        "            image=image_tokens,     # ‚úÖ image token indices (not raw image)\n",
        "            return_loss=True\n",
        ")\n",
        "\n",
        "        # üîÅ backward + optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (step + 1)\n",
        "\n",
        "        # ‚è≥ ETA Í≥ÑÏÇ∞\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "        avg_time_per_step = elapsed / (step + 1)\n",
        "        remaining = max_batches - (step + 1)\n",
        "        eta = timedelta(seconds=int(avg_time_per_step * remaining))\n",
        "        eta_str = (datetime.now() + eta).strftime('%H:%M:%S')\n",
        "        \n",
        "        for path in train_dataset.img_paths[step * batch_size:(step + 1) * batch_size]:\n",
        "            base_name = os.path.splitext(os.path.basename(path))[0]  # Ïòà: 000000000009\n",
        "            txt_path = os.path.join(\"coco_text_pairs\", f\"{base_name}.txt\")\n",
        "            with open(txt_path, 'r') as f:\n",
        "                used_captions.add(f.read().strip())\n",
        "\n",
        "    # epoch ÎÅùÎÇòÍ≥† loss Ï∂úÎ†•\n",
        "    print(f\"[Epoch {epoch+1}] Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # üß† Î™®Îç∏ Ï†ÄÏû•\n",
        "    torch.save(dalle.state_dict(), os.path.join(save_dir, f\"dalle_epoch{epoch+1}.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Ï¥ù ÌïôÏäµÏóê ÏÇ¨Ïö©Îêú Ï∫°ÏÖò Ïàò: {len(used_captions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ÌïôÏäµ ÏôÑÎ£å, Î∂àÎü¨Ïò§Ïûê"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dalle_target_path = 'checkpoints/dalle_epoch1.pt'\n",
        "\n",
        "# ‚úÖ VAE Î°úÎìú\n",
        "vae = load_vae()\n",
        "\n",
        "# ‚úÖ ÌÜµÏùºÎêú DALL¬∑E Íµ¨Ï°∞Î°ú Ï¥àÍ∏∞Ìôî\n",
        "dalle = load_dalle(vae)\n",
        "\n",
        "# ‚úÖ Í∞ÄÏ§ëÏπò Î∂àÎü¨Ïò§Í∏∞\n",
        "dalle.load_state_dict(torch.load(dalle_target_path, map_location=device))\n",
        "dalle.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(type(bpe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"apple\"\n",
        "\n",
        "# ‚úÖ Î¶¨Ïä§Ìä∏Î°ú Í∞êÏã∏ÏÑú Ïù∏ÏΩîÎî©\n",
        "tokens = bpe.encode([prompt], output_type=OutputType.ID)[0]\n",
        "tokens = tokens[:128] + [0] * (128 - len(tokens))\n",
        "\n",
        "text_token = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "# Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±\n",
        "dalle.eval()\n",
        "with torch.no_grad():\n",
        "    generated_images = dalle.generate_images(text_token)\n",
        "\n",
        "# Í≤∞Í≥º ÏãúÍ∞ÅÌôî\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grid = make_grid(generated_images, nrow=1)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "plt.axis(\"off\")\n",
        "plt.title(prompt)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pretrained?? https://github.com/robvanvolt/DALLE-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import youtokentome as yttm\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pad_tokens(token_ids, max_len=256):\n",
        "    if len(token_ids) > max_len:\n",
        "        return token_ids[:max_len]\n",
        "    else:\n",
        "        return token_ids + [0] * (max_len - len(token_ids))\n",
        "def clip_tokens(token_ids, max_token_id):\n",
        "    return [min(t, max_token_id) for t in token_ids]\n",
        "\n",
        "def generate_images_for_models(model_paths, pairs, bpe):\n",
        "    all_results = []\n",
        "\n",
        "    for model_path in model_paths:\n",
        "        print(f\"Loading model: {os.path.basename(model_path)}\")  # Î™®Îç∏Î™Ö Ï∂úÎ†•\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        hparams = checkpoint.get(\"hparams\", {})\n",
        "        state_dict = checkpoint.get(\"weights\", checkpoint)\n",
        "\n",
        "        vae = load_vae()\n",
        "        dalle = DALLE(\n",
        "            vae=vae,\n",
        "            num_text_tokens=hparams.get(\"num_text_tokens\", 10000),\n",
        "            text_seq_len=hparams.get(\"text_seq_len\", 128),\n",
        "            dim=hparams.get(\"dim\", 544),\n",
        "            depth=hparams.get(\"depth\", 12),\n",
        "            heads=hparams.get(\"heads\", 8),\n",
        "            dim_head=hparams.get(\"dim_head\", 64)\n",
        "        )\n",
        "\n",
        "        def load_partial_state_dict(model, checkpoint_state_dict):\n",
        "            model_state_dict = model.state_dict()\n",
        "            filtered_dict = {}\n",
        "            for k, v in checkpoint_state_dict.items():\n",
        "                if k in model_state_dict and v.shape == model_state_dict[k].shape:\n",
        "                    filtered_dict[k] = v\n",
        "            model_state_dict.update(filtered_dict)\n",
        "            model.load_state_dict(model_state_dict)\n",
        "\n",
        "        load_partial_state_dict(dalle, state_dict)\n",
        "\n",
        "        dalle.to(device)\n",
        "        dalle.eval()\n",
        "\n",
        "        max_token_id = hparams.get(\"num_text_tokens\", 10000) - 1\n",
        "\n",
        "        for pair in tqdm(pairs, desc=os.path.basename(model_path)):\n",
        "            prompt = pair[\"prompt\"]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                token_ids = bpe.encode([prompt], output_type=yttm.OutputType.ID)[0]\n",
        "                # ÌÜ†ÌÅ∞ idÍ∞Ä Î≤îÏúÑ Î∞ñÏù¥Î©¥ ÏµúÎåÄ ÌÜ†ÌÅ∞ idÎ°ú ÌÅ¥Î¶¨Ìïë\n",
        "                token_ids = clip_tokens(token_ids, max_token_id)\n",
        "                padded_tokens = pad_tokens(token_ids, max_len=hparams.get(\"text_seq_len\", 128))\n",
        "                text = torch.tensor(padded_tokens).unsqueeze(0).to(device)\n",
        "\n",
        "                generated = dalle.generate_images(text, filter_thres=0.9)\n",
        "                gen_img = transforms.ToPILImage()(generated.squeeze(0).cpu())\n",
        "\n",
        "            print(f\"Model: {os.path.basename(model_path)}\")\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            display(gen_img)  # Ï£ºÌîºÌÑ∞ ÎÖ∏Ìä∏Î∂ÅÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Ï∂úÎ†•\n",
        "\n",
        "            all_results.append({\n",
        "                \"model\": os.path.basename(model_path),\n",
        "                \"prompt\": prompt,\n",
        "                \"image\": gen_img\n",
        "            })\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe = yttm.BPE(model=\"bpe.model\")\n",
        "model_dir = \"pretrainedmodels_collected_dalle\"\n",
        "usable_models = [\n",
        "    \"conceptual_cc_dalle.pt\",\n",
        "    \"dalle_epoch1.pt\",\n",
        "    \"experimental_dalle.pt\",\n",
        "    \"royalty_free_dalle.pt\"\n",
        "]\n",
        "model_paths = [os.path.join(model_dir, m) for m in usable_models]\n",
        "\n",
        "results = generate_images_for_models(model_paths, pairs, bpe)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "train_dalle_pytorch.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dalle-py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
