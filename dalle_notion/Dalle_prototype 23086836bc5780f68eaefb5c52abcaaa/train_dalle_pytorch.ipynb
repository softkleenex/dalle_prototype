{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vyuFJyZ0WrA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pip, import관리, wandia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.') \n",
        "\n",
        "#python 3.9.23\n",
        "\n",
        "# 📦 pip 최신화 (optional)\n",
        "!{sys.executable} -m pip install pip==25.1.1\n",
        "\n",
        "# ✅ PyTorch + torchvision (선택한 버전, CUDA 자동 감지 or CPU용 따로 명시 가능)\n",
        "!{sys.executable} -m pip install torch==2.1.2 torchvision==0.16.2\n",
        "\n",
        "# 🤖 Transformers (안정 버전)\n",
        "!{sys.executable} -m pip install transformers==4.53.1\n",
        "\n",
        "# 🧩 필수 유틸\n",
        "!{sys.executable} -m pip install einops==0.8.1\n",
        "!{sys.executable} -m pip install regex\n",
        "!{sys.executable} -m pip install omegaconf==2.3.0\n",
        "!{sys.executable} -m pip install \"matplotlib>=3.5,<3.9\"\n",
        "!{sys.executable} -m pip install Pillow\n",
        "!{sys.executable} -m pip install numpy\n",
        "\n",
        "# 🔤 BPE Tokenizer\n",
        "!{sys.executable} -m pip install cython\n",
        "!{sys.executable} -m pip install youtokentome\n",
        "\n",
        "# 🧠 OpenAI DALL·E 공식 VAE (.pkl 기반)\n",
        "!{sys.executable} -m pip install git+https://github.com/openai/DALL-E\n",
        "\n",
        "\n",
        "# 🔁 numpy 재설치 및 버전 다운그레이드 (권장 안정 버전)\n",
        "!{sys.executable} -m pip install --force-reinstall numpy==1.24.4\n",
        "\n",
        "!{sys.executable} -m pip install dalle-pytorch taming-transformers-rom1504  \n",
        "\n",
        "!{sys.executable} -m pip install git+https://github.com/openai/CLIP.git           \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기본 유틸\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "from glob import glob\n",
        "import pkg_resources\n",
        "\n",
        "# PyTorch & 툴킷\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# 이미지 및 시각화\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tokenizer\n",
        "import youtokentome as yttm\n",
        "\n",
        "# 패키지 설치 (한 번만 실행)\n",
        "!git clone https://github.com/lucidrains/DALLE-pytorch.git\n",
        "!pip install -e ./DALLE-pytorch\n",
        "!pip install taming-transformers-rom1504 einops omegaconf regex matplotlib Pillow numpy youtokentome\n",
        "\n",
        "# 경로 추가\n",
        "sys.path.append('./DALLE-pytorch')\n",
        "\n",
        "# DALL·E import\n",
        "from dalle_pytorch import DALLE, VQGanVAE\n",
        "\n",
        "# torch 함수들\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from math import sqrt\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from youtokentome import BPE, OutputType \n",
        "\n",
        "import wandb\n",
        "wandb.login(key=\"9f75da6a26ca1502b0b9429b23f6f3f0feef990d\")\n",
        "\n",
        "from youtokentome import OutputType\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import youtokentome as yttm\n",
        "from dalle_pytorch.vae import VQGanVAE as BaseVQGanVAE\n",
        "\n",
        "import clip\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 비교 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_prompts_with_reference_images(images_root='images', image_exts=('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "    data = []\n",
        "\n",
        "    for folder_name in os.listdir(images_root):\n",
        "        folder_path = os.path.join(images_root, folder_name)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        prompt_file = os.path.join(folder_path, 'prompts.txt')\n",
        "        if not os.path.exists(prompt_file):\n",
        "            continue\n",
        "\n",
        "        # 이미지 후보 리스트\n",
        "        image_files = [\n",
        "            os.path.join(folder_path, fname)\n",
        "            for fname in os.listdir(folder_path)\n",
        "            if fname.lower().endswith(image_exts)\n",
        "        ]\n",
        "\n",
        "        if not image_files:\n",
        "            continue  # 이미지 없으면 skip\n",
        "\n",
        "        with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                prompt = line.strip()\n",
        "                if not prompt:\n",
        "                    continue\n",
        "\n",
        "                # 이미지 중 하나 랜덤 선택\n",
        "                ref_image_path = random.choice(image_files)\n",
        "                data.append({\n",
        "                    'prompt': prompt,\n",
        "                    'ref_image': ref_image_path,\n",
        "                    'folder': folder_path\n",
        "                })\n",
        "\n",
        "    return data\n",
        "\n",
        "# ✅ 사용 예시\n",
        "pairs = load_prompts_with_reference_images()\n",
        "print(f\"{len(pairs)}개의 프롬프트-이미지 쌍이 로드되었습니다.\")\n",
        "print(pairs[0])  # {'prompt': ..., 'ref_image': ..., 'folder': ...}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7XIByl3J8fB"
      },
      "source": [
        "### VAE 클래스 정의 및 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class VQGanVAE(BaseVQGanVAE):\n",
        "    def __init__(self, vqgan_model_path=None, vqgan_config_path=None, from_pretrained=True):\n",
        "        super().__init__()\n",
        "        if from_pretrained:\n",
        "            self.load_ckpt_and_config(vqgan_model_path, vqgan_config_path)\n",
        "        else:\n",
        "            self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        pass  # 나중에 .pt만 불러쓸 경우 필요한 구조 정의\n",
        "\n",
        "    def load_ckpt_and_config(self, model_path, config_path):\n",
        "        # 원래 dalle_pytorch에 있는 VQGanVAE.load_ckpt_and_config 메서드 참고\n",
        "        import yaml\n",
        "        from omegaconf import OmegaConf\n",
        "        from taming.models.vqgan import VQModel\n",
        "        from taming.models import cond_transformer, vqgan\n",
        "\n",
        "        # config 불러오기\n",
        "        config = OmegaConf.load(config_path)\n",
        "        model = VQModel(**config.model.params)\n",
        "\n",
        "        # ckpt 로드\n",
        "        sd = torch.load(model_path, map_location='cpu')['state_dict']\n",
        "        model.load_state_dict(sd, strict=False)\n",
        "\n",
        "        # 속성 바인딩\n",
        "        self.model = model\n",
        "        self.image_size = 256\n",
        "        self.num_tokens = 1024\n",
        "        self.num_layers = 4\n",
        "        \n",
        "        \n",
        "def get_codebook_indices(self, img):\n",
        "    z = self.model.encoder(img)\n",
        "    z = self.model.quant_conv(z)\n",
        "    quant, indices, _ = self.model.quantize(z)   # 일반적으로 [B, H, W]\n",
        "\n",
        "    # 🔒 indices shape 점검 및 보정\n",
        "    if indices.ndim == 3:\n",
        "        B, H, W = indices.shape\n",
        "        indices = indices.view(B, -1)   # [B, H*W]\n",
        "    elif indices.ndim == 1:\n",
        "        indices = indices.unsqueeze(0)  # [1, N]\n",
        "    elif indices.ndim == 0:\n",
        "        indices = indices.view(1, 1)    # scalar → [1, 1]\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected indices shape: {indices.shape}\")\n",
        "\n",
        "    return indices  # ✅ 항상 [B, N] 형태 보장\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dalle_pytorch.vae import VQGanVAE  # 동일한 모듈에서 로드되어야 함\n",
        "def load_vae(fast=True, force_yaml=False, debug=True):\n",
        "\n",
        "    # ✅ VAE 로드\n",
        "    if fast and not force_yaml and os.path.exists(\"vqgan_vae.pt\"):\n",
        "        vae = VQGanVAE(\n",
        "            vqgan_model_path = \"vqgan_imagenet_f16_1024/ckpts/last.ckpt\",\n",
        "            vqgan_config_path = \"vqgan_imagenet_f16_1024/configs/model.yaml\"\n",
        "        )\n",
        "        vae.load_state_dict(torch.load(\"vqgan_vae.pt\"))\n",
        "    else:\n",
        "        vae = VQGanVAE(\n",
        "            vqgan_model_path = \"vqgan_imagenet_f16_1024/ckpts/last.ckpt\",\n",
        "            vqgan_config_path = \"vqgan_imagenet_f16_1024/configs/model.yaml\"\n",
        "        )\n",
        "        torch.save(vae.state_dict(), \"vqgan_vae.pt\")\n",
        "\n",
        "    vae.eval()\n",
        "#vae 코드북 사이즈 1024 논문에서는 8192\n",
        "\n",
        "\n",
        "    # ✅ 디버그: 이미지 + 텍스트 → concat 확인\n",
        "    if debug:\n",
        "        # sample 이미지 준비\n",
        "        sample_path = \"sample.png\"\n",
        "        if not os.path.exists(sample_path):\n",
        "            print(f\"⚠️ 샘플 이미지 없음: {sample_path}\")\n",
        "            return vae\n",
        "\n",
        "        # 이미지 전처리\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        image = Image.open(sample_path).convert(\"RGB\")\n",
        "        image_tensor = transform(image).unsqueeze(0)  # [1, 3, 256, 256]\n",
        "\n",
        "        # ✅ 이미지 → 토큰\n",
        "        with torch.no_grad():\n",
        "            image_tokens = vae.get_codebook_indices(image_tensor)  # [1, 1024] or [1024] or scalar 등 다양함\n",
        "\n",
        "        # 안전하게 shape 출력\n",
        "        image_tokens = image_tokens.view(1, -1)  # 강제로 [1, N] 형태로 바꿔줌\n",
        "        print(\"🧠 VAE image token shape:\", image_tokens.shape)\n",
        "        print(\"    예시 토큰 ID:\", image_tokens[0][:10].tolist())\n",
        "\n",
        "        # ✅ 텍스트 → 토큰\n",
        "        bpe = yttm.BPE(model=\"bpe.model\")\n",
        "        prompt = \"a red square\"\n",
        "        text_ids = bpe.encode([prompt], output_type=yttm.OutputType.ID)[0]\n",
        "        text_ids = text_ids[:128] + [0] * (128 - len(text_ids))  # 패딩\n",
        "        text_tensor = torch.tensor(text_ids).unsqueeze(0)  # [1, 128]\n",
        "\n",
        "        print(\"✏️ Text token shape:\", text_tensor.shape)  # [1, 128]\n",
        "\n",
        "        # ✅ concat\n",
        "        full_input = torch.cat([text_tensor, image_tokens], dim=1)  # [1, 128 + 1024]\n",
        "        print(\"📦 Concat된 transformer 입력 shape:\", full_input.shape)  # [1, 1152]\n",
        "\n",
        "    return vae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COCO데이터셋 처리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#COCO 데이터셋을 DALL·E 학습에 쓸 수 있도록 전처리\n",
        "\n",
        "# 경로 설정\n",
        "IMG_DIR = \"train2014\"\n",
        "CAPTION_FILE = \"annotations/captions_train2014.json\"\n",
        "OUTPUT_DIR = \"coco_text_pairs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 캡션 JSON 로드\n",
        "with open(CAPTION_FILE, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# image_id → caption 매핑 (첫 캡션만 사용)\n",
        "id_to_caption = {}\n",
        "for ann in data['annotations']:\n",
        "    img_id = ann['image_id']\n",
        "    caption = ann['caption']\n",
        "    if img_id not in id_to_caption:\n",
        "        id_to_caption[img_id] = caption\n",
        "\n",
        "# 변환 루프\n",
        "skipped, converted = 0, 0\n",
        "\n",
        "for img in tqdm(data['images'], desc=\"Converting COCO\"):\n",
        "    img_id = img['id']\n",
        "    filename = img['file_name']\n",
        "    if img_id not in id_to_caption:\n",
        "        continue\n",
        "\n",
        "    src_img = os.path.join(IMG_DIR, filename)\n",
        "    base = f\"{img_id:012d}\"\n",
        "    dst_img = os.path.join(OUTPUT_DIR, f\"{base}.jpg\")\n",
        "    dst_txt = os.path.join(OUTPUT_DIR, f\"{base}.txt\")\n",
        "\n",
        "    # ✅ 이미 변환된 경우 건너뛰기\n",
        "    if os.path.exists(dst_img) and os.path.exists(dst_txt):\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    if os.path.exists(src_img):\n",
        "        shutil.copy(src_img, dst_img)\n",
        "        with open(dst_txt, \"w\") as f:\n",
        "            f.write(id_to_caption[img_id])\n",
        "        converted += 1\n",
        "\n",
        "# 결과 요약 출력\n",
        "print(f\"✅ 변환 완료: {converted}개 생성, {skipped}개는 이미 존재해서 건너뜀.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"captions.txt\"):\n",
        "    with open(\"captions.txt\", \"w\") as out:\n",
        "        for txt_file in glob(\"coco_text_pairs/*.txt\"):\n",
        "            with open(txt_file, \"r\") as f:\n",
        "                line = f.read().strip()\n",
        "                out.write(line + \"\\n\")\n",
        "    print(\"✅ captions.txt 생성 완료\")\n",
        "else:\n",
        "    print(\"✅ captions.txt 이미 존재함 — 건너뜀\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### bpe 모델(caption(text) 2 token) 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#bpe모델 < 문장의 분할 모델\n",
        "#논문 BPE 모델 재현, \n",
        "yttm.BPE.train(\n",
        "    data='captions.txt',         # 전체 캡션 텍스트 파일\n",
        "    model='bpe.model',\n",
        "    vocab_size=16384\n",
        ")\n",
        "#bpe.model완성\n",
        "\n",
        "bpe = yttm.BPE(model=\"bpe.model\")\n",
        "#토크나이저 불러온\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CocoTextPairDataset(Dataset):\n",
        "    \n",
        "    \n",
        "    def __init__(self, folder, tokenizer=None, transform=None, text_seq_len=128, dropout_prob=0.1):#논문에서의 bpe 10% drop 재현\n",
        "        self.folder = folder\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_seq_len = text_seq_len\n",
        "        self.transform = transform\n",
        "        self.dropout_prob = dropout_prob  # ⬅️ 추가\n",
        "        self.img_paths = []\n",
        "\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if file.endswith('.jpg') and 'images' not in os.path.join(root, file):\n",
        "                    self.img_paths.append(os.path.join(root, file))\n",
        "        self.img_paths.sort()\n",
        "  \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        base_name = os.path.splitext(img_path)[0]\n",
        "        txt_path = f\"{base_name}.txt\"\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        with open(txt_path, 'r') as f:\n",
        "            caption = f.read().strip()\n",
        "\n",
        "        if self.tokenizer:\n",
        "            token_ids = self.tokenizer.encode(\n",
        "                [caption],\n",
        "                output_type=yttm.OutputType.ID,\n",
        "                dropout_prob=self.dropout_prob  # 🎯 논문 BPE dropout\n",
        "            )[0]\n",
        "            \n",
        "            # 🔧 길이 고정 + 패딩\n",
        "            token_ids = token_ids[:self.text_seq_len]\n",
        "            token_ids += [0] * (self.text_seq_len - len(token_ids))\n",
        "            \n",
        "            text = torch.tensor(token_ids)\n",
        "        else:\n",
        "            text = None\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#논문 이미지 증강 재현\n",
        "class DalleImageAugmentation:\n",
        "    def __init__(self, target_res=256):\n",
        "        self.target_res = target_res\n",
        "\n",
        "    def __call__(self, img):  # img: PIL.Image\n",
        "        from torchvision.transforms import functional as TF\n",
        "        import random\n",
        "\n",
        "        w, h = img.size\n",
        "        s_min = min(w, h)\n",
        "\n",
        "        # Random crop offset\n",
        "        off_h = random.randint(3 * (h - s_min) // 8, max(5 * (h - s_min) // 8, 3 * (h - s_min) // 8 + 1))\n",
        "        off_w = random.randint(3 * (w - s_min) // 8, max(5 * (w - s_min) // 8, 3 * (w - s_min) // 8 + 1))\n",
        "\n",
        "        # Square crop\n",
        "        img = TF.crop(img, top=off_h, left=off_w, height=s_min, width=s_min)\n",
        "\n",
        "        # Resize to random size between [target_res, 9/8 * target_res]\n",
        "        t_max = min(s_min, round(9 / 8 * self.target_res))\n",
        "        t = random.randint(self.target_res, t_max)\n",
        "        img = TF.resize(img, [t, t], interpolation=TF.InterpolationMode.BILINEAR)\n",
        "\n",
        "        # Final random crop to target size\n",
        "        img = TF.center_crop(img, [self.target_res, self.target_res])\n",
        "        img = TF.to_tensor(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe = yttm.BPE(model=\"bpe.model\")\n",
        "\n",
        "\n",
        "transform = DalleImageAugmentation(target_res=256)\n",
        "\n",
        "\n",
        "# 학습용\n",
        "train_dataset = CocoTextPairDataset(\n",
        "    folder=\"coco_text_pairs\",\n",
        "    tokenizer=bpe,\n",
        "    transform=transform,\n",
        "    text_seq_len=128,\n",
        "    dropout_prob=0.1  # 🎯 이게 핵심\n",
        ")\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "# ✅ 추론/검증용 (BPE dropout 없음)\n",
        "eval_dataset = CocoTextPairDataset(\n",
        "    folder=\"coco_text_pairs\",\n",
        "    tokenizer=bpe,\n",
        "    transform=transform,\n",
        "    text_seq_len=128,\n",
        "    dropout_prob=0.0\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import youtokentome as yttm\n",
        "\n",
        "print(bpe.vocab_size())  # => 16384가 나와야 논문과 동일\n",
        "\n",
        "\n",
        "bpe = yttm.BPE(model='bpe.model')\n",
        "print(\"📏 BPE vocab size:\", bpe.vocab_size())\n",
        "\n",
        "vae = load_vae()\n",
        "print(\"🎨 Codebook size:\", vae.codebook.n_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 트랜스포머 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# 💡 VAE는 freeze (미리 학습된 것 사용)\n",
        "vae.eval()\n",
        "for param in vae.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 💾 체크포인트 저장 경로\n",
        "save_dir = \"checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ⚙️ 학습 설정\n",
        "dalle.train()\n",
        "optimizer = torch.optim.Adam(dalle.parameters(), lr=1e-4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#조금만 학습해보자!\n",
        "num_epochs = 1\n",
        "max_batches = 250\n",
        "batch_size = 4\n",
        "used_captions = set()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    start_time = datetime.now()  # ⏱️ ETA 계산용 시작 시각\n",
        "    pbar = tqdm(enumerate(train_dataloader), total= max_batches, desc = f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for step, (images, tokenized_texts) in pbar:\n",
        "        \n",
        "        if step >= max_batches:  # 🔒 제한된 step까지만 학습\n",
        "            break\n",
        "\n",
        "        # GPU 또는 MPS 할당\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            image_tokens = vae.get_codebook_indices(images)  # ✅ shape: [B, 1024]\n",
        "\n",
        "        loss = dalle(\n",
        "            text=tokenized_texts,\n",
        "            image=image_tokens,     # ✅ image token indices (not raw image)\n",
        "            return_loss=True\n",
        ")\n",
        "\n",
        "        # 🔁 backward + optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (step + 1)\n",
        "\n",
        "        # ⏳ ETA 계산\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "        avg_time_per_step = elapsed / (step + 1)\n",
        "        remaining = max_batches - (step + 1)\n",
        "        eta = timedelta(seconds=int(avg_time_per_step * remaining))\n",
        "        eta_str = (datetime.now() + eta).strftime('%H:%M:%S')\n",
        "        \n",
        "        for path in train_dataset.img_paths[step * batch_size:(step + 1) * batch_size]:\n",
        "            base_name = os.path.splitext(os.path.basename(path))[0]  # 예: 000000000009\n",
        "            txt_path = os.path.join(\"coco_text_pairs\", f\"{base_name}.txt\")\n",
        "            with open(txt_path, 'r') as f:\n",
        "                used_captions.add(f.read().strip())\n",
        "\n",
        "    # epoch 끝나고 loss 출력\n",
        "    print(f\"[Epoch {epoch+1}] Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 🧠 모델 저장\n",
        "    torch.save(dalle.state_dict(), os.path.join(save_dir, f\"dalle_epoch{epoch+1}.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"총 학습에 사용된 캡션 수: {len(used_captions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 학습 완료, 불러오자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dalle_target_path = 'checkpoints/dalle_epoch1.pt'\n",
        "\n",
        "# ✅ VAE 로드\n",
        "vae = load_vae()\n",
        "\n",
        "# ✅ 통일된 DALL·E 구조로 초기화\n",
        "dalle = load_dalle(vae)\n",
        "\n",
        "# ✅ 가중치 불러오기\n",
        "dalle.load_state_dict(torch.load(dalle_target_path, map_location=device))\n",
        "dalle.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(type(bpe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"apple\"\n",
        "\n",
        "# ✅ 리스트로 감싸서 인코딩\n",
        "tokens = bpe.encode([prompt], output_type=OutputType.ID)[0]\n",
        "tokens = tokens[:128] + [0] * (128 - len(tokens))\n",
        "\n",
        "text_token = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "# 이미지 생성\n",
        "dalle.eval()\n",
        "with torch.no_grad():\n",
        "    generated_images = dalle.generate_images(text_token)\n",
        "\n",
        "# 결과 시각화\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grid = make_grid(generated_images, nrow=1)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "plt.axis(\"off\")\n",
        "plt.title(prompt)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pretrained?? https://github.com/robvanvolt/DALLE-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import youtokentome as yttm\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pad_tokens(token_ids, max_len=256):\n",
        "    if len(token_ids) > max_len:\n",
        "        return token_ids[:max_len]\n",
        "    else:\n",
        "        return token_ids + [0] * (max_len - len(token_ids))\n",
        "def clip_tokens(token_ids, max_token_id):\n",
        "    return [min(t, max_token_id) for t in token_ids]\n",
        "\n",
        "def generate_images_for_models(model_paths, pairs, bpe):\n",
        "    all_results = []\n",
        "\n",
        "    for model_path in model_paths:\n",
        "        print(f\"Loading model: {os.path.basename(model_path)}\")  # 모델명 출력\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        hparams = checkpoint.get(\"hparams\", {})\n",
        "        state_dict = checkpoint.get(\"weights\", checkpoint)\n",
        "\n",
        "        vae = load_vae()\n",
        "        dalle = DALLE(\n",
        "            vae=vae,\n",
        "            num_text_tokens=hparams.get(\"num_text_tokens\", 10000),\n",
        "            text_seq_len=hparams.get(\"text_seq_len\", 128),\n",
        "            dim=hparams.get(\"dim\", 544),\n",
        "            depth=hparams.get(\"depth\", 12),\n",
        "            heads=hparams.get(\"heads\", 8),\n",
        "            dim_head=hparams.get(\"dim_head\", 64)\n",
        "        )\n",
        "\n",
        "        def load_partial_state_dict(model, checkpoint_state_dict):\n",
        "            model_state_dict = model.state_dict()\n",
        "            filtered_dict = {}\n",
        "            for k, v in checkpoint_state_dict.items():\n",
        "                if k in model_state_dict and v.shape == model_state_dict[k].shape:\n",
        "                    filtered_dict[k] = v\n",
        "            model_state_dict.update(filtered_dict)\n",
        "            model.load_state_dict(model_state_dict)\n",
        "\n",
        "        load_partial_state_dict(dalle, state_dict)\n",
        "\n",
        "        dalle.to(device)\n",
        "        dalle.eval()\n",
        "\n",
        "        max_token_id = hparams.get(\"num_text_tokens\", 10000) - 1\n",
        "\n",
        "        for pair in tqdm(pairs, desc=os.path.basename(model_path)):\n",
        "            prompt = pair[\"prompt\"]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                token_ids = bpe.encode([prompt], output_type=yttm.OutputType.ID)[0]\n",
        "                # 토큰 id가 범위 밖이면 최대 토큰 id로 클리핑\n",
        "                token_ids = clip_tokens(token_ids, max_token_id)\n",
        "                padded_tokens = pad_tokens(token_ids, max_len=hparams.get(\"text_seq_len\", 128))\n",
        "                text = torch.tensor(padded_tokens).unsqueeze(0).to(device)\n",
        "\n",
        "                generated = dalle.generate_images(text, filter_thres=0.9)\n",
        "                gen_img = transforms.ToPILImage()(generated.squeeze(0).cpu())\n",
        "\n",
        "            print(f\"Model: {os.path.basename(model_path)}\")\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "            display(gen_img)  # 주피터 노트북에서 이미지 출력\n",
        "\n",
        "            all_results.append({\n",
        "                \"model\": os.path.basename(model_path),\n",
        "                \"prompt\": prompt,\n",
        "                \"image\": gen_img\n",
        "            })\n",
        "\n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bpe = yttm.BPE(model=\"bpe.model\")\n",
        "model_dir = \"pretrainedmodels_collected_dalle\"\n",
        "usable_models = [\n",
        "    \"conceptual_cc_dalle.pt\",\n",
        "    \"dalle_epoch1.pt\",\n",
        "    \"experimental_dalle.pt\",\n",
        "    \"royalty_free_dalle.pt\"\n",
        "]\n",
        "model_paths = [os.path.join(model_dir, m) for m in usable_models]\n",
        "\n",
        "results = generate_images_for_models(model_paths, pairs, bpe)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "train_dalle_pytorch.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dalle-py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
