# Dalle_prototype

í•´ë‹¹ êµ¬í˜„ì€ í¬ê²Œ 3ê°€ì§€ë¡œ ë‚˜ëˆŒìˆ˜ìˆë‹¤

BPE

VAE 

TRANSFORM

### 1. BPE

![image.png](asset/image.png)

```python
class CocoTextPairDataset(Dataset):
    
    
    def __init__(self, folder, tokenizer=None, transform=None, text_seq_len=128, dropout_prob=0.1):#ë…¼ë¬¸ì—ì„œì˜ bpe 10% drop ì¬í˜„
        self.folder = folder
        self.tokenizer = tokenizer
        self.text_seq_len = text_seq_len
        self.transform = transform
        self.dropout_prob = dropout_prob  # â¬…ï¸ ì¶”ê°€
        self.img_paths = []

        for root, _, files in os.walk(folder):
            for file in files:
                if file.endswith('.jpg') and 'images' not in os.path.join(root, file):
                    self.img_paths.append(os.path.join(root, file))
        self.img_paths.sort()
  
        
    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        base_name = os.path.splitext(img_path)[0]
        txt_path = f"{base_name}.txt"

        image = Image.open(img_path).convert("RGB")

        with open(txt_path, 'r') as f:
            caption = f.read().strip()

        if self.tokenizer:
            token_ids = self.tokenizer.encode(
                [caption],
                output_type=yttm.OutputType.ID,
                dropout_prob=self.dropout_prob  # ğŸ¯ ë…¼ë¬¸ BPE dropout
            )[0]
            
            # ğŸ”§ ê¸¸ì´ ê³ ì • + íŒ¨ë”©
            token_ids = token_ids[:self.text_seq_len]
            token_ids += [0] * (self.text_seq_len - len(token_ids))
            
            text = torch.tensor(token_ids)
        else:
            text = None

        if self.transform:
            image = self.transform(image)

        return image, text
```

ë…¼ë¬¸ì—ì˜ 10% ë“œëì•„ì›ƒ ì¬í˜„ë¨.

```python
    def __init__(self, folder, tokenizer=None, transform=None, text_seq_len=128, dropout_prob=0.1):#ë…¼ë¬¸ì—ì„œì˜ bpe 10% drop ì¬í˜„
        self.folder = folder
        self.tokenizer = tokenizer
        self.text_seq_len = text_seq_len
        self.transform = transform
        self.dropout_prob = dropout_prob  # â¬…ï¸ ì¶”ê°€
        self.img_paths = []

        for root, _, files in os.walk(folder):
            for file in files:
                if file.endswith('.jpg') and 'images' not in os.path.join(root, file):
                    self.img_paths.append(os.path.join(root, file))
        self.img_paths.sort()
```

![image.png](asset/image-1.png)

vocab_size ì¬í˜„ ì™„ë£Œ.(16384)

ì¦‰, BPEëŠ” êµ¬í˜„ ì™„ë£Œ.

### VAE

![image.png](asset/image-2.png)

```python
#ë…¼ë¬¸ ì´ë¯¸ì§€ ì¦ê°• ì¬í˜„
class DalleImageAugmentation:
    def __init__(self, target_res=256):
        self.target_res = target_res

    def __call__(self, img):
        from torchvision.transforms import functional as TF
        import random

        w, h = img.size
        s_min = min(w, h)

        # âœ… ìµœì†Œ í¬ê¸° ì²´í¬
        if s_min < self.target_res:
            img = TF.resize(img, [self.target_res, self.target_res])
            return TF.to_tensor(img)

        # ë‚˜ë¨¸ì§€ ê¸°ì¡´ ë¡œì§ ìœ ì§€
        off_h = random.randint(3 * (h - s_min) // 8, max(5 * (h - s_min) // 8, 3 * (h - s_min) // 8 + 1))
        off_w = random.randint(3 * (w - s_min) // 8, max(5 * (w - s_min) // 8, 3 * (w - s_min) // 8 + 1))

        img = TF.crop(img, top=off_h, left=off_w, height=s_min, width=s_min)

        t_max = min(s_min, round(9 / 8 * self.target_res))
        if t_max < self.target_res:
            t = self.target_res
        else:
            t = random.randint(self.target_res, t_max)

        img = TF.resize(img, [t, t], interpolation=TF.InterpolationMode.BILINEAR)
        img = TF.center_crop(img, [self.target_res, self.target_res])
        img = TF.to_tensor(img)
        return img
```

ë…¼ë¬¸ê³¼ ê°™ì€ open_aiì˜ vaeë¥¼ ì´ìš©í•˜ëŠ”ê²ƒì€

1. íŒŒì´í† ì¹˜ ë“± ë‹¤ë¥¸ ëª¨ë“ˆì˜ ì˜¤ë˜ëœ ë²„ì „ì„ ê°•ìš”í•˜ê³  ìˆì–´, ì´ìš©ì´ ì–´ë ¤ì›€
2. ë‹¨ìˆœ apië¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ê²ƒìœ¼ë¡œ íŒŒì¸íŠœë‹ ë¶ˆê°€, íŒŒì¼ ì¡°ì • ë¶ˆê°€ > í•™ìŠµì—ëŠ” ë¶€ì ì ˆí•¨

ë”°ë¼ì„œ .yaml, ckptë¡œ ë°›ì•„ ì‚¬ìš©í• ìˆ˜ ìˆëŠ” ì €ìˆ˜ì¤€ vaeì´ìš©

1. ê·¸ë ‡ì§€ë§Œ, ì´ë¯¸ì§€ ìƒì„±ì—ëŠ” ì˜í–¥ì´ ìˆì„ìˆ˜ ìˆìŒ, íŠ¹íˆ ì½”ë“œë¶ ì‚¬ì´ì¦ˆ í™•ì¸ í•„ìš”(ë…¼ë¬¸ì—ì„œëŠ” 8192)

![image.png](asset//image-3.png)

1. ê·¸ë ‡ì§€ë§Œ í˜„ì¬ì˜ vaeë„ ì˜ ì•Œë ¤ì§„ vaeë¥¼ ì‚¬ìš©ì¤‘, ê°€ëŠ¥í•˜ë‹¤ë©´ ì´ê²ƒ ì‚¬ìš©í•  ê³„íš.

[https://github.com/CompVis/taming-transformers](https://github.com/CompVis/taming-transformers)

ë‹¨, .yamlíŒŒì¼ì˜ ì½”ë“œë¶ í¬ê¸°ê°€ ë…¼ë¬¸ê³¼ ë‹¬ë¼,  ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©ì¤‘

```python
model:
  base_learning_rate: 4.5e-06
  target: taming.models.vqgan.VQModel
  params:
    embed_dim: 256
    n_embed: 8192 #1024 > 8192, ë…¼ë¬¸ ì½”ë“œë¶ í¬ê¸° ì¬í˜„
    ddconfig:
      double_z: false
      z_channels: 256
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 1
      - 2
      - 2
      - 4
      num_res_blocks: 2
      attn_resolutions:
      - 16
      dropout: 0.0
    lossconfig:
      target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
      params:
        disc_conditional: false
        disc_in_channels: 3
        disc_start: 0
        disc_weight: 0.8
        codebook_weight: 1.0
        

```

1. vaeëŠ” ë³´í†µ frozenëœ í›„ì—, í•™ìŠµë˜ì§€ ì•ŠëŠ”ê²ƒì´ ê´€í–‰ìœ¼ë¡œ ë³´ì¸ë‹¤

https://github.com/lucidrains/DALLE-pytorch/discussions/375

![image.png](asset/image-4.png)

í˜„ì¬ ì‚¬ìš©ì¤‘ì¸ vaeëŠ” ë¯¿ì„ ë§Œí•˜ê¸°ë•Œë¬¸ì—, êµ¬ì¡°ìˆ˜ì • ì™¸ì˜ í•™ìŠµì€ ê³„íší•˜ì§€ ì•Šì•˜ë‹¤.

### íŠ¸ëœìŠ¤í¬ë¨¸

```python
#ì¡°ê¸ˆë§Œ í•™ìŠµí•´ë³´ì!
num_epochs = 1
max_batches = 250
batch_size = 4
used_captions = set()

for epoch in range(num_epochs):
    total_loss = 0.0
    start_time = datetime.now()  # â±ï¸ ETA ê³„ì‚°ìš© ì‹œì‘ ì‹œê°
    pbar = tqdm(enumerate(train_dataloader), total= max_batches, desc = f"Epoch {epoch+1}")

    for step, (images, tokenized_texts) in pbar:
        
        if step >= max_batches:  # ğŸ”’ ì œí•œëœ stepê¹Œì§€ë§Œ í•™ìŠµ
            break

        # GPU ë˜ëŠ” MPS í• ë‹¹
        images = images.to(device)
        with torch.no_grad():
            image_tokens = vae.get_codebook_indices(images)  # âœ… shape: [B, 1024]

        loss = dalle(
            text=tokenized_texts,
            image=image_tokens,     # âœ… image token indices (not raw image)
            return_loss=True
)

        # ğŸ” backward + optimizer step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        avg_loss = total_loss / (step + 1)

        # â³ ETA ê³„ì‚°
        elapsed = (datetime.now() - start_time).total_seconds()
        avg_time_per_step = elapsed / (step + 1)
        remaining = max_batches - (step + 1)
        eta = timedelta(seconds=int(avg_time_per_step * remaining))
        eta_str = (datetime.now() + eta).strftime('%H:%M:%S')
        
        for path in train_dataset.img_paths[step * batch_size:(step + 1) * batch_size]:
            base_name = os.path.splitext(os.path.basename(path))[0]  # ì˜ˆ: 000000000009
            txt_path = os.path.join("coco_text_pairs", f"{base_name}.txt")
            with open(txt_path, 'r') as f:
                used_captions.add(f.read().strip())

    # epoch ëë‚˜ê³  loss ì¶œë ¥
    print(f"[Epoch {epoch+1}] Average Loss: {avg_loss:.4f}")

    # ğŸ§  ëª¨ë¸ ì €ì¥
    torch.save(dalle.state_dict(), os.path.join(save_dir, f"dalle_epoch{epoch+1}.pt"))
```

ê¸°ë³¸ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ì„¤ê³„ ì™„ë£Œ,

[dalle_epoch1.pt](dalle_notion%Dalle_prototype%2023086836bc5780f68eaefb5c52abcaaa/dalle_epoch1.pt)

epoch1ìœ¼ë¡œ í•™ìŠµ ë° .pt(ì¬í˜„ ê°€ëŠ¥í•œ  dalleëª¨ë¸) ìƒì„± ì™„ë£Œ,

ê·¸ë ‡ì§€ë§Œ ì´ë¯¸ì§€ ìƒì„±ì€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ìˆ˜ì¤€

![image.png](asset/image-7.png)

Why?

bpe, vae, transformerì„ ì‚´í´ë³´ë©´,,

vaeëŠ” ì˜ ì•Œë ¤ì§„ íŒŒì¼ ì´ìš©

https://github.com/CompVis/taming-transformers?tab=readme-ov-file

BPEëŠ” 

ë…¼ë¬¸ê³¼ ê°™ì´ ë“œë¡­ì•„ì›ƒ ì ìš©ì¤‘, ë…¼ë¬¸ì—ì„œë„ ê¸°ë³¸ì ì¸ BPEë§Œ ì´ìš©ì¤‘

íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡° / í•™ìŠµëŸ‰ì˜ ë¬¸ì œë¡œ íŒë‹¨ >

DALLE-modelsì¤‘ì— ë¯¸ë¦¬ í•™ìŠµëœ ë°ì´í„°ëŠ” ì—†ì„ê¹Œ? .ptíŒŒì¼ë¡œ,,,

https://github.com/robvanvolt/DALLE-models

ì´ 4ê°œ íŒŒì¼ í™•ë³´ ë° ì‚¬ìš© ê²°ê³¼

>BAD

![alt text](asset/image-8.png)

![alt text](asset/image-9.png)

![alt text](asset/image-10.png)

![alt text](asset/image-11.png)

>êµ³ì´ ì‚¬ìš©í•  í•„ìš” ì—†ëŠ”ë“¯, ë‚´ ëª¨ë¸ë¡œ ê³„ì† ê°€ë³´ì,

ì¶”í›„ ìƒê° >

íŠ¸ëœìŠ¤í¬ë¨¸ ì„¤ê³„ ë…¼ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë°”ê¾¸ê¸°,

í˜„ì¬ëŠ” ì½”ë“œì˜ ì •ìƒ ì¢…ë£Œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ìˆ˜ì¤€ìœ¼ë¡œ, ë§¤ìš° ê°„ë‹¨í•˜ê²Œ ì„¤ê³„í•œ ìƒíƒœ, 

ë…¼ë¬¸ì˜ êµ¬ì¡° ë°˜ì˜ ë° íŒŒë¼ë¯¸í„° ë“± í™•ì¸í•´ë³´ì.

![image.png](asset//image-6.png)

1. í•™ìŠµ ë§ì´ ëŒë¦¬ê¸°(ìµœì†Œí•œ 6ì‹œê°„ì •ë„) >ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” gpu í™˜ê²½ìœ¼ë¡œ ë°”ê¾¸ê¸°!

[train_dalle_pytorch.ipynb](train_dalle_pytorch.ipynb)