<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Zero-Shot Text-to-Image Generation</title>
<!--Generated on Fri Jul  4 15:05:41 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<link rel="stylesheet" href="ltx-listings.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Zero-Shot Text-to-Image Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aditya Ramesh
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mikhail Pavlov
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gabriel Goh
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Scott Gray
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chelsea Voss
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alec Radford
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mark Chen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ilya Sutskever
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.</p>
  
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Modern machine learning approaches to text to image synthesis started with the work of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mansimov2015generating</span></cite>, who showed that the DRAW <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">gregor2015draw</span></cite> generative model, when extended to condition on image captions, could also generate novel visual scenes. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">reed2016generative</span></cite> later demonstrated that using a generative adversarial network <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">goodfellow2014generative</span>)</cite>, rather than a recurrent variational auto-encoder, improved image fidelity. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">reed2016generative</span></cite> showed that this system could not only generate objects with recognizable properties, but also could <span class="ltx_text ltx_font_italic">zero-shot</span> generalize to held-out categories.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Over the next few years, progress continued using a combination of methods. These include improving the generative model architecture with modifications like multi-scale generators <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2017stackgan</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2018stackgan++</span>)</cite>, integrating attention and auxiliary losses <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2018attngan</span>)</cite>, and leveraging additional sources of conditioning information beyond just text <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">reed2016learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li2019object</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">koh2021text</span>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="dvae_rec.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of original images (top) and reconstructions from the discrete VAE (bottom). The encoder downsamples the spatial resolution by a factor of 8. While details (e.g., the texture of the cat’s fur, the writing on the storefront, and the thin lines in the illustration) are sometimes lost or distorted, the main features of the image are still typically recognizable. We use a large vocabulary size of 8192 to mitigate the loss of information.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Separately, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen2017plug</span></cite> propose an energy-based framework for conditional image generation that obtained a large improvement in sample quality relative to contemporary methods. Their approach can incorporate pretrained discriminative models, and they show that it is capable of performing text-to-image generation when applied to a captioning model pretrained on MS-COCO.
More recently, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">cho2020x</span></cite> also propose a method that involves optimizing the input to a pretrained cross-modal masked language model. While significant increases in visual fidelity have occurred as a result of the work since <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mansimov2015generating</span></cite>, samples can still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Recent advances fueled by large-scale generative models suggest a possible route for further improvements. Specifically, when compute, model size, and data are scaled carefully, autoregressive transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">vaswani2017attention</span>)</cite> have achieved impressive results in several domains such as text <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">radford2019language</span>)</cite>, images <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2020generative</span>)</cite>, and audio <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">dhariwal2020jukebox</span>)</cite>.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="tapir_0.png" id="S1.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="tapir_1.png" id="S1.F2.sf1.g2" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="tapir_6.png" id="S1.F2.sf1.g3" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="tapir_14.png" id="S1.F2.sf1.g4" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>a tapir made of accordion. a tapir with the texture of an accordion.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="hedgehog_1.png" id="S1.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="hedgehog_3.png" id="S1.F2.sf2.g2" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="hedgehog_16.png" id="S1.F2.sf2.g3" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="hedgehog_17.png" id="S1.F2.sf2.g4" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>an illustration of a baby hedgehog in a christmas sweater walking a dog</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="10.png" id="S1.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="11.png" id="S1.F2.sf3.g2" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="13.png" id="S1.F2.sf3.g3" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="14.png" id="S1.F2.sf3.g4" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>a neon sign that reads “backprop”. a neon sign that reads “backprop”. backprop neon sign</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="30.png" id="S1.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="31.png" id="S1.F2.sf4.g2" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="32.png" id="S1.F2.sf4.g3" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="33.png" id="S1.F2.sf4.g4" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>the exact same cat on the top as a sketch on the bottom</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>With varying degrees of reliability, our model appears to be able to combine distinct concepts in plausible ways, create anthropomorphized versions of animals, render text, and perform some types of image-to-image translation.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">By comparison, text-to-image generation has typically been evaluated on relatively small datasets such as MS-COCO and CUB-200 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">welinder2010caltech</span>)</cite>. Could dataset size and model size be the limiting factor of current approaches? In this work, we demonstrate that training a 12-billion parameter autoregressive transformer on 250 million image-text pairs collected from the internet results in a flexible, high fidelity generative model of images controllable through natural language.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">The resulting system achieves high quality image generation on the popular MS-COCO dataset <span class="ltx_text ltx_font_italic">zero-shot</span>, without using any of the training labels. It is preferred over prior work trained on the dataset by human evaluators 90% of the time. We also find that it is able to perform complex tasks such as image-to-image translation at a rudimentary level. This previously required custom approaches <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">isola2017image</span>)</cite>, rather
emerging as a capability of a single, large generative model.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="coco_cmp_v2.jpg" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of samples from our model to those from prior approaches on captions from MS-COCO. Each of our model samples is the best of 512 as ranked by the contrastive model. We do not use any manual cherrypicking with the selection of either the captions or the samples from any of the models.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Our goal is to train a transformer <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">vaswani2017attention</span>)</cite> to autoregressively model the text and image tokens as a single stream of data. However, using pixels directly as image tokens would require an inordinate amount of memory for high-resolution images. Likelihood objectives tend to prioritize modeling short-range dependencies between pixels <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">salimans2017pixelcnn++</span>)</cite>, so much of the modeling capacity would be spent capturing high-frequency details instead of the low-frequency structure that makes objects visually recognizable to us.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">We address these issues by using a two-stage training procedure, similar to <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">oord2017neural</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">razavi2019generating</span>)</cite>:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Stage 1.</span> We train a discrete variational autoencoder (dVAE)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
                  <span class="ltx_tag ltx_tag_note">1</span>
                  
                  
                  
                <a href="https://github.com/openai/DALL-E" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/DALL-E</a></span></span></span> to compress each <math id="S2.I1.i1.p1.m1" class="ltx_Math" alttext="256\times 256" display="inline"><mrow><mn>256</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>256</mn></mrow></math> RGB image into a <math id="S2.I1.i1.p1.m2" class="ltx_Math" alttext="32\times 32" display="inline"><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow></math> grid of image tokens, each element of which can assume <math id="S2.I1.i1.p1.m3" class="ltx_Math" alttext="8192" display="inline"><mn>8192</mn></math> possible values. This reduces the context size of the transformer by a factor of <math id="S2.I1.i1.p1.m4" class="ltx_Math" alttext="192" display="inline"><mn>192</mn></math> without a large degradation in visual quality (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Stage 2.</span> We concatenate up to 256 BPE-encoded text tokens with the <math id="S2.I1.i2.p1.m1" class="ltx_Math" alttext="32\times 32=1024" display="inline"><mrow><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow><mo>=</mo><mn>1024</mn></mrow></math> image tokens, and train an autoregressive transformer to model the joint distribution over the text and image tokens.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">The overall procedure can be viewed as maximizing the evidence lower bound (ELB) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kingma2013auto</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">rezende2014stochastic</span>)</cite> on the joint likelihood of the model distribution over images <math id="S2.p3.m1" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math>, captions <math id="S2.p3.m2" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>, and the tokens <math id="S2.p3.m3" class="ltx_Math" alttext="z" display="inline"><mi>z</mi></math> for the encoded RGB image. We model this distribution using the factorization <math id="S2.p3.m4" class="ltx_Math" alttext="p_{\theta,\psi}(x,y,z)=p_{\theta}(x\,|\,y,z)p_{\psi}(y,z)" display="inline"><mrow><mrow><msub><mi>p</mi><mrow><mi>θ</mi><mo>,</mo><mi>ψ</mi></mrow></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo fence="false" lspace="0.448em" rspace="0.448em">|</mo><mrow><mi>y</mi><mo>,</mo><mi>z</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><msub><mi>p</mi><mi>ψ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, which yields the lower bound</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1" class="ltx_Math" alttext="\ln p_{\theta,\psi}(x,y)\geqslant\!\!\!\!\!\!\!\!\mathop{\mathbb{E}}_{\begin{%
subarray}{c}\vspace{0.1mm}\\
z\sim q_{\phi}(z\,|\,x)\end{subarray}}\!\!\!\!\!\!\!\!\big{(}\ln p_{\theta}(x%
\,|\,y,z)\;-\\
\beta\,D_{\mathrm{KL}}(q_{\phi}(y,z\,|\,x),p_{\psi}(y,z))\big{)}," display="block"><mtable displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mi>ln</mi><msub><mi>p</mi><mrow><mi>θ</mi><mo>,</mo><mi>ψ</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mpadded width="0em"><mo>⩾</mo></mpadded><munder><mo movablelimits="false" rspace="0em">𝔼</mo><mtable rowspacing="0pt"><mtr><mtd></mtd></mtr><mtr><mtd><mrow><mi>z</mi><mo>∼</mo><mrow><msub><mi>q</mi><mi>ϕ</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo fence="false" lspace="0.448em" rspace="0.448em">|</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mtd></mtr></mtable></munder><mrow><mo maxsize="120%" minsize="120%">(</mo><mi>ln</mi><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mi>x</mi><mo fence="false" lspace="0.170em" rspace="0.337em" stretchy="false">|</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo rspace="0.280em" stretchy="false">)</mo></mrow><mo>−</mo></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><mi>β</mi><msub><mi>D</mi><mi>KL</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo fence="false" lspace="0.170em" rspace="0.337em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><msub><mi>p</mi><mi>ψ</mi></msub><mrow><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">)</mo><mo>,</mo></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p class="ltx_p"><math id="S2.I2.i1.p1.m1" class="ltx_Math" alttext="q_{\phi}" display="inline"><msub><mi>q</mi><mi>ϕ</mi></msub></math> denotes the distribution over the <math id="S2.I2.i1.p1.m2" class="ltx_Math" alttext="32\times 32" display="inline"><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow></math> image tokens generated by the dVAE encoder given the RGB image <math id="S2.I2.i1.p1.m3" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>
                  <span class="ltx_tag ltx_tag_note">2</span>
                  
                  
                  
                We assume that <math id="footnote2.m1" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> is conditionally independent of <math id="footnote2.m2" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> given <math id="footnote2.m3" class="ltx_Math" alttext="z" display="inline"><mi>z</mi></math>.</span></span></span>;</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p class="ltx_p"><math id="S2.I2.i2.p1.m1" class="ltx_Math" alttext="p_{\theta}" display="inline"><msub><mi>p</mi><mi>θ</mi></msub></math> denotes the distribution over the RGB images generated by the dVAE decoder given the image tokens; and</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p class="ltx_p"><math id="S2.I2.i3.p1.m1" class="ltx_Math" alttext="p_{\psi}" display="inline"><msub><mi>p</mi><mi>ψ</mi></msub></math> denotes the joint distribution over the text and image tokens modeled by the transformer.</p>
</div>
</li>
</ul>
<p class="ltx_p">Note that the bound only holds for <math id="S2.p3.m5" class="ltx_Math" alttext="\beta=1" display="inline"><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow></math>, while in practice we find it helpful to use larger values <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">higgins2016beta</span>)</cite>. The following subsections describe both stages in further detail.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>
            <span class="ltx_tag ltx_tag_note">3</span>
            
            
            
          In preliminary experiments on ImageNet <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">deng2009imagenet</span>)</cite>, we attempted to maximize the ELB with respect to <math id="footnote3.m1" class="ltx_Math" alttext="\phi" display="inline"><mi>ϕ</mi></math>, <math id="footnote3.m2" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>, and <math id="footnote3.m3" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math> jointly, but were unable to improve on two-stage training.</span></span></span></p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Stage One: Learning the Visual Codebook</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">In the first stage of training, we maximize the ELB with respect to <math id="S2.SS1.p1.m1" class="ltx_Math" alttext="\phi" display="inline"><mi>ϕ</mi></math> and <math id="S2.SS1.p1.m2" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>, which corresponds to training a dVAE on the images alone. We set the initial prior <math id="S2.SS1.p1.m3" class="ltx_Math" alttext="p_{\psi}" display="inline"><msub><mi>p</mi><mi>ψ</mi></msub></math> to the uniform categorical distribution over the <math id="S2.SS1.p1.m4" class="ltx_Math" alttext="K=$8192$" display="inline"><mrow><mi>K</mi><mo>=</mo><mn>8192</mn></mrow></math> codebook vectors, and <math id="S2.SS1.p1.m5" class="ltx_Math" alttext="q_{\phi}" display="inline"><msub><mi>q</mi><mi>ϕ</mi></msub></math> to be categorical distributions parameterized by the <math id="S2.SS1.p1.m6" class="ltx_Math" alttext="8192" display="inline"><mn>8192</mn></math> logits at the same spatial position in the <math id="S2.SS1.p1.m7" class="ltx_Math" alttext="32\times 32" display="inline"><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow></math> grid output by the encoder.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">The ELB now becomes difficult to optimize: as <math id="S2.SS1.p2.m1" class="ltx_Math" alttext="q_{\psi}" display="inline"><msub><mi>q</mi><mi>ψ</mi></msub></math> is a discrete distribution, and we cannot use the reparameterization gradient to maximize it. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">oord2017neural</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">razavi2019generating</span></cite> address this using an online cluster assignment procedure coupled with the straight-through estimator <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">bengio2013estimating</span>)</cite>. We instead use the gumbel-softmax relaxation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">jang2016categorical</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">maddison2016concrete</span>)</cite>, replacing the expectation over <math id="S2.SS1.p2.m2" class="ltx_Math" alttext="q_{\phi}" display="inline"><msub><mi>q</mi><mi>ϕ</mi></msub></math> with one over <math id="S2.SS1.p2.m3" class="ltx_Math" alttext="q^{\tau}_{\phi}" display="inline"><msubsup><mi>q</mi><mi>ϕ</mi><mi>τ</mi></msubsup></math>, where the relaxation becomes tight as the temperature <math id="S2.SS1.p2.m4" class="ltx_Math" alttext="\tau\to 0" display="inline"><mrow><mi>τ</mi><mo stretchy="false">→</mo><mn>0</mn></mrow></math>. The likelihood for <math id="S2.SS1.p2.m5" class="ltx_Math" alttext="p_{\theta}" display="inline"><msub><mi>p</mi><mi>θ</mi></msub></math> is evaluated using the log-laplace distribution (see Appendix <a href="#A1.SS3" title="A.3 The Logit-Laplace Distribution ‣ Appendix A Details for Discrete VAE ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a> for a derivation).</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">The relaxed ELB is maximized using Adam <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">kingma2014adam</span>)</cite> with exponentially weighted iterate averaging. Appendix <a href="#A1.SS2" title="A.2 Training ‣ Appendix A Details for Discrete VAE ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a> gives a complete description of the hyperparameters, but we found the following to be especially important for stable training:</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p class="ltx_p">Specific annealing schedules for the relaxation temperature and step size. We found that annealing <math id="S2.I3.i1.p1.m1" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> to <math id="S2.I3.i1.p1.m2" class="ltx_Math" alttext="1/16" display="inline"><mrow><mn>1</mn><mo>/</mo><mn>16</mn></mrow></math> was sufficient to close the gap between the relaxed validation ELB and the true validation ELB with <math id="S2.I3.i1.p1.m3" class="ltx_Math" alttext="q_{\phi}" display="inline"><msub><mi>q</mi><mi>ϕ</mi></msub></math> intsead of <math id="S2.I3.i1.p1.m4" class="ltx_Math" alttext="q_{\phi}^{\tau}" display="inline"><msubsup><mi>q</mi><mi>ϕ</mi><mi>τ</mi></msubsup></math>.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p class="ltx_p">The use of <math id="S2.I3.i2.p1.m1" class="ltx_Math" alttext="1\times 1" display="inline"><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></math> convolutions at the end of the encoder and the beginning of the decoder. We found that reducing the receptive field size for the convolutions around the relaxation led to it generalizing better to the true ELB.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p class="ltx_p">Multiplication of the outgoing activations from the encoder and decoder resblocks by a small constant, to ensure stable training at initialization.</p>
</div>
</li>
</ul>
<p class="ltx_p">We also found that increasing the KL weight to <math id="S2.SS1.p3.m1" class="ltx_Math" alttext="\beta=6.6" display="inline"><mrow><mi>β</mi><mo>=</mo><mn>6.6</mn></mrow></math> promotes better codebook usage and ultimately leads to a <em class="ltx_emph ltx_font_italic">smaller</em> reconstruction error at the end of training.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>
              <span class="ltx_tag ltx_tag_note">4</span>
              
              
              
            This is contrary to the usual tradeoff between the two terms. We speculate that for smaller values of <math id="footnote4.m1" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math>, the noise from the relaxation causes the optimizer to reduce codebook usage toward the beginning of training, resulting in worse ELB at convergence.</span></span></span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Stage Two: Learning the Prior</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">In the second stage, we fix <math id="S2.SS2.p1.m1" class="ltx_Math" alttext="\phi" display="inline"><mi>ϕ</mi></math> and <math id="S2.SS2.p1.m2" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>, and learn the prior distribution over the text and image tokens by maximizing the ELB with respect to <math id="S2.SS2.p1.m3" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math>. Here, <math id="S2.SS2.p1.m4" class="ltx_Math" alttext="p_{\psi}" display="inline"><msub><mi>p</mi><mi>ψ</mi></msub></math> is represented by a 12-billion parameter sparse transformer <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">child2019generating</span>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">Given a text-image pair, we BPE-encode <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sennrich2015neural</span>)</cite> the lowercased caption using at most 256 tokens<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>
              <span class="ltx_tag ltx_tag_note">5</span>
              
              
              
            During training, we apply 10% BPE dropout <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">provilkov2019bpe</span>)</cite>, whose use is common in the neural machine translation literature.</span></span></span> with vocabulary size <math id="S2.SS2.p2.m1" class="ltx_Math" alttext="16384" display="inline"><mn>16384</mn></math>, and encode the image using <math id="S2.SS2.p2.m2" class="ltx_Math" alttext="32\times 32=1024" display="inline"><mrow><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow><mo>=</mo><mn>1024</mn></mrow></math> tokens with vocabulary size <math id="S2.SS2.p2.m3" class="ltx_Math" alttext="8192" display="inline"><mn>8192</mn></math>. The image tokens are obtained using argmax sampling from the dVAE encoder logits, without adding any gumbel noise.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>
              <span class="ltx_tag ltx_tag_note">6</span>
              
              
              
            Strictly speaking, Equation <a href="#S2.E1" title="In 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> requires us to sample from the categorical distribution specified by the dVAE encoder logits, rather than taking the argmax. In preliminary experiments on ImageNet, we found that this was a useful regularizer in the overparameterized regime, and allows the transformer to be trained using soft targets for the cross-entropy loss. We decided against this here since the model in consideration is in the underparameterized regime.</span></span></span> Finally, the text and image tokens are concatenated and modeled autoregressively as a single stream of data.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">The transformer is a decoder-only model in which each image token can attend to all text tokens in any one of its 64 self-attention layers. The full architecture is described in Appendix <a href="#A2.SS1" title="B.1 Architecture ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>. There are three different kinds of self-attention masks used in the model. The part of the attention masks corresponding to the text-to-text attention is the standard causal mask, and the part for the image-to-image attention uses either a row, column, or convolutional attention mask.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup>
              <span class="ltx_tag ltx_tag_note">7</span>
              
              
              
            We found using a single attention operation for all three interactions – “text attends to text”, “image attends to text”, and “image attends to image” – to perform better than using separate attention operations that are independently normalized.</span></span></span></p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">We limit the length of a text caption to 256 tokens, though it is not totally clear what to do for the “padding” positions in between the last text token and the start-of-image token. One option is to set the logits for these tokens to <math id="S2.SS2.p4.m1" class="ltx_Math" alttext="-\infty" display="inline"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></math> in the self-attention operations. Instead, we opt to learn a special padding token separately for each of the 256 text positions. This token is used only when no text token is available. In preliminary experiments on Conceptual Captions <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2018conceptual</span>)</cite>, we found that this resulted in higher validation loss, but better performance on out-of-distribution captions.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p class="ltx_p">We normalize the cross-entropy losses for the text and image tokens by the total number of each kind in a batch of data. Since we are primarily interested in image modeling, we multiply the cross-entropy loss for the text by <math id="S2.SS2.p5.m1" class="ltx_Math" alttext="1/8" display="inline"><mrow><mn>1</mn><mo>/</mo><mn>8</mn></mrow></math> and the cross-entropy loss for the image by <math id="S2.SS2.p5.m2" class="ltx_Math" alttext="7/8" display="inline"><mrow><mn>7</mn><mo>/</mo><mn>8</mn></mrow></math>. The objective is optimized using Adam with exponentially weighted iterate averaging; Appendix <a href="#A2.SS2" title="B.2 Training ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> describes the training procedure in more detail. We reserved about <math id="S2.SS2.p5.m3" class="ltx_Math" alttext="606000" display="inline"><mn>606000</mn></math> images for validation, and found no signs of overfitting at convergence.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="per_resblock_scaling.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of per-resblock gradient scaling for a transformer resblock. The solid line indicates the sequence of operations for forward propagation, and the dashed line the sequence of operations for backpropagation. We scale the incoming gradient for each resblock by its gradient scale, and unscale the outgoing gradient before it is added to the sum of the gradients from the successive resblocks. The activations and gradients along the identity path are stored in 32-bit precision. The “filter” operation sets all Inf and NaN values in the activation gradient to zero. Without this, a nonfinite event in the current resblock would cause the gradient scales for all preceding resblocks to unnecessarily drop, thereby resulting in underflow.</figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="dist_comm.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Communication patterns used for distributed training. Each parameter array in the model is sharded among the eight GPUs on each machine. During forward propagation, we prefetch the parameter shards for the next resblock (using all-gather) while computing the activations for the current resblock. To conserve memory, the parameter shards from the other GPUs are immediately discarded. Similarly, during backpropagation, we prefetch the parameter shards for the previous resblock while computing the activations and gradients for the current resblock. After all GPUs have computed the gradient with respect to an all-gathered parameter, the reduce-scatter operation leaves each GPU with only one slice – i.e., the gradient for its parameter shard, averaged over the eight GPUs.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data Collection</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">Our preliminary experiments for models up to <math id="S2.SS3.p1.m1" class="ltx_Math" alttext="1.2" display="inline"><mn>1.2</mn></math> billion parameters were carried out on Conceptual Captions, a dataset of 3.3 million text-image pairs that was developed as an extension to MS-COCO <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">lin2014microsoft</span>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">To scale up to <math id="S2.SS3.p2.m1" class="ltx_Math" alttext="12" display="inline"><mn>12</mn></math>-billion parameters, we created a dataset of a similar scale to JFT-300M <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sun2017revisiting</span>)</cite> by collecting 250 million text-images pairs from the internet. This dataset does not include MS-COCO, but does include Conceptual Captions and a filtered subset of YFCC100M <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">thomee2016yfcc100m</span>)</cite>. As MS-COCO was created from the latter, our training data includes a fraction of the MS-COCO validation images (but none of the captions). We control for this in the quantitative results presented in Section <a href="#S3" title="3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and find that it has no appreciable bearing on the results. We provide further details about the data collection process in Appendix <a href="#A3" title="Appendix C Details for Data Collection ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Mixed-Precision Training</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">To save GPU memory and increase throughput, most parameters, Adam moments, and activations are stored in 16-bit precision. We also use activation checkpointing and recompute the activations within the resblocks during the backward pass. Getting the model to train in 16-bit precision past one billion parameters, without diverging, was the most challenging part of this project.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p class="ltx_p">We believe the root cause of this instability to be underflow in the 16-bit gradients. Appendix <a href="#A4" title="Appendix D Guidelines for Mixed-Precision Training ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> presents a set of guidelines we developed to avoid underflow when training large-scale generative models. Here, we describe one of these guidelines: per-resblock gradient scaling.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p class="ltx_p">Similar to prior work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2020understanding</span>)</cite>, we found that the norms of the activation gradients from the resblocks decrease monotonically as we move from the earlier resblocks to the later ones.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup>
              <span class="ltx_tag ltx_tag_note">8</span>
              
              
              
            It is possible that better initialization schemes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2020understanding</span>)</cite> might be able to avoid this, but we did not have success with alternative schemes in our experiments.</span></span></span> As the model is made deeper and wider, the true exponents of the activation gradients for later resblocks can fall below the minimum exponent of the 16-bit format. Consequently, they get rounded to zero, a phenomenon called <em class="ltx_emph ltx_font_italic">underflow</em>. We found that eliminating underflow allowed for stable training to convergence.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p class="ltx_p">Standard loss scaling <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">micikevicius2017mixed</span>)</cite> is able to avoid underflow when the range spanned by the smallest and largest activation gradients (in absolute value) fits within the exponent range of the 16-bit format. On NVIDIA V100 GPUs, this exponent range is specified by five bits. While this is sufficient for training vanilla language models of the same size, we found the range to be too small for the text-to-image model.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p class="ltx_p">Our fix, which is shown in Figure <a href="#S2.F4" title="Figure 4 ‣ 2.2 Stage Two: Learning the Prior ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, involves using a separate “gradient scale” for each resblock in the model. This can be seen as a practical alternative to a more general framework for mixed-precision training called Flexpoint <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">koster2017flexpoint</span>)</cite>, with the advantage that specialized GPU kernels are not required. We found that <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sun2020ultra</span></cite> had independently developed similar procedure for training convolutional networks in 4-bit precision.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Distributed Optimization</h3>

<figure id="S2.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text" style="font-size:70%;">Effective Parameter Count</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:70%;">Compression Rank</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:70%;">Compression Rate</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<math id="S2.T1.m1" class="ltx_Math" alttext="2.8\cdot 10^{9}" display="inline"><mrow><mn mathsize="70%">2.8</mn><mo lspace="0.222em" mathsize="70%" rspace="0.222em">⋅</mo><msup><mn mathsize="70%">10</mn><mn mathsize="70%">9</mn></msup></mrow></math><span class="ltx_text" style="font-size:70%;"> (</span><math id="S2.T1.m2" class="ltx_Math" alttext="d_{\mathrm{model}}=1920" display="inline"><mrow><msub><mi mathsize="70%">d</mi><mi mathsize="70%">model</mi></msub><mo mathsize="70%">=</mo><mn mathsize="70%">1920</mn></mrow></math><span class="ltx_text" style="font-size:70%;">)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:70%;">512</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="S2.T1.m3" class="ltx_Math" alttext="\approx\!83\%" display="inline"><mrow><mi></mi><mo mathsize="70%" rspace="0.108em">≈</mo><mrow><mn mathsize="70%">83</mn><mo mathsize="70%">%</mo></mrow></mrow></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">
<math id="S2.T1.m4" class="ltx_Math" alttext="5.6\cdot 10^{9}" display="inline"><mrow><mn mathsize="70%">5.6</mn><mo lspace="0.222em" mathsize="70%" rspace="0.222em">⋅</mo><msup><mn mathsize="70%">10</mn><mn mathsize="70%">9</mn></msup></mrow></math><span class="ltx_text" style="font-size:70%;"> (</span><math id="S2.T1.m5" class="ltx_Math" alttext="d_{\mathrm{model}}=2688" display="inline"><mrow><msub><mi mathsize="70%">d</mi><mi mathsize="70%">model</mi></msub><mo mathsize="70%">=</mo><mn mathsize="70%">2688</mn></mrow></math><span class="ltx_text" style="font-size:70%;">)</span>
</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="font-size:70%;">640</span></td>
<td class="ltx_td ltx_align_center"><math id="S2.T1.m6" class="ltx_Math" alttext="\approx\!85\%" display="inline"><mrow><mi></mi><mo mathsize="70%" rspace="0.108em">≈</mo><mrow><mn mathsize="70%">85</mn><mo mathsize="70%">%</mo></mrow></mrow></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<math id="S2.T1.m7" class="ltx_Math" alttext="12.0\cdot 10^{9}" display="inline"><mrow><mn mathsize="70%">12.0</mn><mo lspace="0.222em" mathsize="70%" rspace="0.222em">⋅</mo><msup><mn mathsize="70%">10</mn><mn mathsize="70%">9</mn></msup></mrow></math><span class="ltx_text" style="font-size:70%;"> (</span><math id="S2.T1.m8" class="ltx_Math" alttext="d_{\mathrm{model}}=3968" display="inline"><mrow><msub><mi mathsize="70%">d</mi><mi mathsize="70%">model</mi></msub><mo mathsize="70%">=</mo><mn mathsize="70%">3968</mn></mrow></math><span class="ltx_text" style="font-size:70%;">)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="font-size:70%;">896</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math id="S2.T1.m9" class="ltx_Math" alttext="\approx\!86\%" display="inline"><mrow><mi></mi><mo mathsize="70%" rspace="0.108em">≈</mo><mrow><mn mathsize="70%">86</mn><mo mathsize="70%">%</mo></mrow></mrow></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>We show the relationship between model size and the minimum compression rank for the gradients (up to a multiple of 128) necessary to avoid a gap in the training loss during the first <math id="S2.T1.m12" class="ltx_Math" alttext="10\%" display="inline"><mrow><mn>10</mn><mo>%</mo></mrow></math> of training. These results suggest that in our setting, we can achieve a compression rate of about <math id="S2.T1.m13" class="ltx_Math" alttext="85\%" display="inline"><mrow><mn>85</mn><mo>%</mo></mrow></math>, independent of model size.</figcaption>
</figure>
<figure id="S2.F6" class="ltx_figure"><img src="coco_reranking.jpg" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="312" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Effect of increasing the number of images for the contrastive reranking procedure on MS-COCO captions.</figcaption>
</figure>
<div id="S2.SS5.p1" class="ltx_para">
<p class="ltx_p">Our 12-billion parameter model consumes about 24 GB of memory when stored in 16-bit precision, which exceeds the memory of a 16 GB NVIDIA V100 GPU. We address this using parameter sharding <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajbhandari2019zero</span>)</cite>. As shown in Figure <a href="#S2.F5" title="Figure 5 ‣ 2.2 Stage Two: Learning the Prior ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, parameter sharding allows us to almost completely hide the latency of the intra-machine communication by overlapping it with compute-intensive operations.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p class="ltx_p">On the cluster used to train the model, the bandwidth between machines is much lower than the bandwidth among GPUs on the same machine. This makes the cost of the operation used to average the gradient among the machines (all-reduce) the main bottleneck during training. We were able to drastically reduce this cost by compressing the gradients using PowerSGD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">vogels2019powersgd</span>)</cite>.</p>
</div>
<div id="S2.SS5.p3" class="ltx_para">
<p class="ltx_p">In our implementation, each GPU in a machine computes the low-rank factors for its parameter shard gradients independently of its neighboring GPUs.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup>
              <span class="ltx_tag ltx_tag_note">9</span>
              
              
              
            There is still intra-machine communication for other operations; what we mean is that the low-rank factors across the shards, when concatenated, are not regarded as collectively approximating the gradient for the full parameter matrix.</span></span></span> Once the low-rank factors are computed, each machine sets its error buffer to the residual between the uncompressed gradient averaged over its eight GPUs (obtained from reduce-scatter), and the decompressed gradient obtained from the low-rank factors.</p>
</div>
<div id="S2.SS5.p4" class="ltx_para">
<p class="ltx_p">PowerSGD replaces the large communication operation for an uncompressed parameter gradient with two, much smaller communication operations for its low-rank factors. For a given compression rank <math id="S2.SS5.p4.m1" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math> and transformer activation size <math id="S2.SS5.p4.m2" class="ltx_Math" alttext="d_{\mathrm{model}}" display="inline"><msub><mi>d</mi><mi>model</mi></msub></math>, the compression rate is given by <math id="S2.SS5.p4.m3" class="ltx_Math" alttext="1-5r/(8d_{\textrm{model}})" display="inline"><mrow><mn>1</mn><mo>−</mo><mrow><mrow><mn>5</mn><mo>⁢</mo><mi>r</mi></mrow><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mn>8</mn><mo>⁢</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> (see Appendix <a href="#A5.SS1" title="E.1 Bandwidth Analysis ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E.1</span></a>). Table <a href="#S2.T1" title="Table 1 ‣ 2.5 Distributed Optimization ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that we can achieve a compression rate of about <math id="S2.SS5.p4.m4" class="ltx_Math" alttext="85\%" display="inline"><mrow><mn>85</mn><mo>%</mo></mrow></math>, independent of model size.</p>
</div>
<div id="S2.SS5.p5" class="ltx_para">
<p class="ltx_p">In Appendix <a href="#A5.SS2" title="E.2 Implementation Details ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E.2</span></a>, we describe various details that were necessary to get PowerSGD to perform well at scale. These include:</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p class="ltx_p">Saving memory by accumulating the gradient into the error buffers during backpropagation, rather than allocating separate buffers.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p class="ltx_p">Minimizing instances in which we zero out the error buffers (e.g., due to nonfinite values encountered during mixed-precision backpropagation, or when resuming training from a checkpoint).</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p class="ltx_p">Improving numerical stability by using Householder orthogonalization instead of Gram-Schmidt, together with the addition of a small multiple of the identity matrix to the input.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p class="ltx_p">Avoiding underflow by using a custom 16-bit floating point format for the error buffers, their low-rank factors, and the all-reduce communication operations involving them.</p>
</div>
</li>
</ul>
<p class="ltx_p">We also found the warm-start procedure for the <math id="S2.SS5.p5.m1" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrix described in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">vogels2019powersgd</span></cite> to be unnecessary: we were able to get equivalent results by fixing <math id="S2.SS5.p5.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> to a random gaussian matrix at the start of training, and never updating it.<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>
              <span class="ltx_tag ltx_tag_note">10</span>
              
              
              
            We verified that the error in reconstructing the true gradient is higher when <math id="footnote10.m1" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> is fixed as opposed to being updated using warm-starting, so it is interesting that this does not affect the loss. By contrast, resampling <math id="footnote10.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> at every update causes a large performance hit.</span></span></span></p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Sample Generation</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">razavi2019generating</span></cite>, we rerank the samples drawn from the transformer using a pretrained contrastive model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">radford2021learning</span>)</cite>. Given a caption and a candidate image, the contrastive model assigns a score based on how well the image matches the caption. Figure <a href="#S2.F6" title="Figure 6 ‣ 2.5 Distributed Optimization ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the effect of increasing the number of samples <math id="S2.SS6.p1.m1" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> from which we select the top <math id="S2.SS6.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> images. This process can be seen as a kind of language-guided search <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">andreas2017learning</span>)</cite>, and is also similar to the auxiliary text-image matching loss proposed by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2018attngan</span></cite>. Unless otherwise stated, all samples used for both qualitative and quantitative results are obtained without temperature reduction (i.e., using <math id="S2.SS6.p1.m3" class="ltx_Math" alttext="t=1" display="inline"><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow></math>) (except for Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and use reranking with <math id="S2.SS6.p1.m4" class="ltx_Math" alttext="N=512" display="inline"><mrow><mi>N</mi><mo>=</mo><mn>512</mn></mrow></math>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<figure id="S3.F7" class="ltx_figure"><img src="assets_v2_final_graph.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Human evaluation of our model (evaluated zero-shot without temperature reduction) vs prior work (DF-GAN) on captions from MS-COCO. In a best-of-five vote, our model’s sample was chosen as the most realistic 90.0% of the time, and was chosen as the image best matching a shared caption 93.3% of the time.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Quantitative Results</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">We evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2018attngan</span>)</cite>, DM-GAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu2019dm</span>)</cite>, and DF-GAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">tao2020df</span>)</cite>, the last of which reports the best Inception Score <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">salimans2016improved</span>)</cite> and Fréchet Inception Distance <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">heusel2017gans</span>)</cite> on MS-COCO. Figure <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> qualitatively compares samples from our model to those from prior work.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">We also conduct a human evaluation similar to the one used in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">koh2021text</span></cite> to compare our approach to DF-GAN, the results of which are shown in Figure <a href="#S3.F7" title="Figure 7 ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Given a caption, the sample from our model receives the majority vote for better matching the caption 93% of the time. It also receives the majority vote for being more realistic 90% of the time.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F9" title="Figure 9 ‣ 3.1 Quantitative Results ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(a) shows that our model also obtains an FID score on MS-COCO within 2 points of the best prior approach, despite having never been trained on the captions. Our training data incorporates a filtered subset of YFCC100M, and we found that it includes about <math id="S3.SS1.p3.m1" class="ltx_Math" alttext="21\%" display="inline"><mrow><mn>21</mn><mo>%</mo></mrow></math> of the images in the MS-COCO validation set from a de-duplication procedure described in the next section. To isolate this effect, we compute the FID statistics for the validation set both with these images (solid lines) and without them (dashed lines), finding no significant change in the results.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">Training the transformer on the tokens from the dVAE encoder allows us to allocate its modeling capacity to the low-frequency information that makes images visually recognizable to us. However, it also disadvantages the model, since the heavy compression renders it unable to produce high-frequency details. To test the effect of this on the quantitative evaluations, we compute the FID and IS in Figure <a href="#S3.F9" title="Figure 9 ‣ 3.1 Quantitative Results ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(a) after applying a Gaussian filter with varying radius to both the validation images and samples from the models. Our approach achieves the best FID by a margin of about 6 points with a slight blur of radius 1. The gap between our approach and others tends to widen as the blur radius is increased. We also obtain the highest IS when the blur radius is greater than or equal to two.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="cub_samples.jpg" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Zero-shot samples from our model on the CUB dataset.</figcaption>
</figure>
<figure id="S3.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="coco_fid.png" id="S3.F9.sf1.g1" class="ltx_graphics ltx_img_landscape" width="186" height="118" alt="Refer to caption"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="coco_is.png" id="S3.F9.sf1.g2" class="ltx_graphics ltx_img_landscape" width="186" height="118" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>FID and IS on MS-COCO as a function of blur radius.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="cub_fid.png" id="S3.F9.sf2.g1" class="ltx_graphics ltx_img_landscape" width="186" height="118" alt="Refer to caption"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="cub_is.png" id="S3.F9.sf2.g2" class="ltx_graphics ltx_img_landscape" width="186" height="118" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>FID and IS on CUB as a function of blur radius.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F9.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="coco_fid_clip_sample_size.png" id="S3.F9.sf3.g1" class="ltx_graphics ltx_img_landscape" width="186" height="118" alt="Refer to caption"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img src="coco_is_clip_sample_size.png" id="S3.F9.sf3.g2" class="ltx_graphics ltx_img_landscape" width="186" height="118" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>FID and IS on MS-COCO as a function of the sample size used for reranking.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Quantitative results on MS-COCO and CUB. Solid lines represent FID computed against the original validation sets, and dashed lines represent FID computed against validation sets with overlapping images removed (see Section <a href="#S3.SS2" title="3.2 Data Overlap Analysis ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). For MS-COCO, we evaluate all models on a subset of <math id="S3.F9.m2" class="ltx_Math" alttext="30000" display="inline"><mn>30000</mn></math> captions sampled from the validation set. For CUB, we evaluate all models on all of the unique captions in the test set. We compute the FID and IS using the DM-GAN code, which is available at <a href="https://github.com/MinfengZhu/DM-GAN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MinfengZhu/DM-GAN</a>.</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p">Our model fares significantly worse on the CUB dataset, for which there is a nearly 40-point gap in FID between our model and the leading prior approach (Figure <a href="#S3.F9" title="Figure 9 ‣ 3.1 Quantitative Results ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(b)). We found an <math id="S3.SS1.p5.m1" class="ltx_Math" alttext="12\%" display="inline"><mrow><mn>12</mn><mo>%</mo></mrow></math> overlap rate for this dataset, and again observed no significant difference in the results after removing these images. We speculate that our zero-shot approach is less likely to compare favorably on specialized distributions such as CUB. We believe that fine-tuning is a promising direction for improvement, and leave this investigation to future work. Samples from our model for captions in this dataset are shown in Figure <a href="#S3.F8" title="Figure 8 ‣ 3.1 Quantitative Results ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p class="ltx_p">Finally, Figure <a href="#S3.F9" title="Figure 9 ‣ 3.1 Quantitative Results ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(c) shows clear improvements in FID and IS for MS-COCO as the sample size used for reranking with the contrastive model is increased. This trend continues up to a sample size of 32, after which we observe diminishing returns.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Overlap Analysis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">We used the deduplication procedure described in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">radford2021learning</span></cite> to determine which images to remove. For each validation image, we find the closest image in the training data using a contrastive model specifically trained for this task. We then sort the images in descending order by closeness to their nearest matches in the training data. After inspecting the results by hand, we determine the images to remove by manually selecting a conservative threshold designed to minimize the false negative rate.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Qualitative Findings</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">We found that our model has the ability to generalize in ways that we did not originally anticipate. When given the caption “a tapir made of accordion…” (Figure <a href="#S1.F2.sf1" title="In Figure 2 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>), the model appears to draw a tapir with an accordion for a body, or an accordion whose keyboard or bass are in the shape of a tapir’s trunk or legs. This suggests that it has developed a rudimentary ability to compose unusual concepts at high levels of abstraction.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p">Our model also appears to be capable of combinatorial generalization, such as when rendering text (Figure <a href="#S1.F2.sf2" title="In Figure 2 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a>) or when probed on sentences like “an illustration of a baby hedgehog in a christmas sweater walking a dog” (Figure <a href="#S1.F2.sf3" title="In Figure 2 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2c</span></a>). Prompts like the latter require the model to perform variable binding <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">smolensky1990tensor</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">greff2020binding</span>)</cite> – it is the hedgehog that is in the christmas sweater, not the dog. We note, however, that the model performs inconsistently on the task, sometimes drawing both animals with christmas sweaters, or drawing a hedgehog walking a smaller hedgehog.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p">To a limited degree of reliability, we also find our model to be capable of zero-shot image-to-image translation controllable by natural language (Figure <a href="#S1.F2.sf4" title="In Figure 2 ‣ 1 Introduction ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2d</span></a>). When the model is given the caption “the exact same cat on the top as a sketch at the bottom” and the top <math id="S3.SS3.p3.m1" class="ltx_Math" alttext="15\times 32" display="inline"><mrow><mn>15</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow></math> part of the image token grid for a photo of a cat, it is able to draw a sketch of a similar looking cat on the bottom.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p class="ltx_p">This works with several other kinds of transformations, including image operations (e.g., changing the color of the image, converting it to grayscale, or flipping it upside-down) and style transfer (e.g., drawing the cat on a greeting card, a postage stamp, or a cell phone case). Some transformations, such as those that involve only changing the color of the animal, suggest that the model is capable of performing a rudimentary kind of object segmentation. We provide additional examples of zero-shot image-to-image translation in Section <a href="#A7" title="Appendix G Zero-Shot Image-to-Image Translation ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We investigate a simple approach for text-to-image generation based on an autoregressive transformer, when it is executed at scale. We find that scale can lead to improved generalization, both in terms of zero-shot performance relative to previous domain-specific approaches, and in terms of the range of capabilities that emerge from a single generative model. Our findings suggest that improving generalization as a function of scale may be a useful driver for progress on this task.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We would like to thank Matthew Knight for reviewing the code release for this work, and Rewon Child, John Schulman, Heewoo Jun, and Prafulla Dhariwal for helpful early feedback on the paper. We would also like to thank Jong Wook Kim for writing the PyTorch package for the contrastive model described in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">radford2019language</span></cite> that we used to rerank the samples from our model.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details for Discrete VAE</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Architecture</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p class="ltx_p">The dVAE encoder and decoder are convolutional <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">lecun1998gradient</span>)</cite> ResNets <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">he2016identity</span>)</cite> with bottleneck-style resblocks. The models primarily use <math id="A1.SS1.p1.m1" class="ltx_Math" alttext="3\times 3" display="inline"><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></math> convolutions, with <math id="A1.SS1.p1.m2" class="ltx_Math" alttext="1\times 1" display="inline"><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></math> convolutions along skip connections in which the number of feature maps changes between the input and output of a resblock. The first convolution of the encoder is <math id="A1.SS1.p1.m3" class="ltx_Math" alttext="7\times 7" display="inline"><mrow><mn>7</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>7</mn></mrow></math>, and the last convolution of the encoder (which produces the <math id="A1.SS1.p1.m4" class="ltx_Math" alttext="32\times 32\times 8192" display="inline"><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>8192</mn></mrow></math> output used as the logits for the categorical distributions for the image tokens) is <math id="A1.SS1.p1.m5" class="ltx_Math" alttext="1\times 1" display="inline"><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></math>. Both the first and last convolutions of the decoder are <math id="A1.SS1.p1.m6" class="ltx_Math" alttext="1\times 1" display="inline"><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow></math>. The encoder uses max-pooling (which we found to yield better ELB than average-pooling) to downsample the feature maps, and the decoder uses nearest-neighbor upsampling. The precise details for the architectures are given in the files <span class="ltx_text ltx_font_typewriter">dvae/encoder.py</span> and <span class="ltx_text ltx_font_typewriter">dvae/decoder.py</span> of the code release.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Training</h3>

<figure id="LST1" class="ltx_float ltx_lstlisting">
<div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,ZGVmIHByZXByb2Nlc3NfaW1hZ2UoaW1nLCB0YXJnZXRfcmVzKToKICAgIGgsIHcgID0gdGYuc2hhcGUoaW1nKVswXSwgdGYuc2hhcGUoaW1nKVsxXQogICAgc19taW4gPSB0Zi5taW5pbXVtKGgsIHcpCiAgICBpbWcgICA9IHRmLmltYWdlLnJhbmRvbV9jcm9wKGltZywgMiAqIFtzX21pbl0gKyBbM10pCgogICAgdF9taW4gPSB0Zi5taW5pbXVtKHNfbWluLCByb3VuZCg5IC8gOCAqIHRhcmdldF9yZXMpKQogICAgdF9tYXggPSB0Zi5taW5pbXVtKHNfbWluLCByb3VuZCgxMiAvIDggKiB0YXJnZXRfcmVzKSkKICAgIHQgICAgID0gdGYucmFuZG9tLnVuaWZvcm0oW10sIHRfbWluLCB0X21heCArIDEsIGR0eXBlPXRmLmludDMyKQogICAgaW1nICAgPSB0Zi5pbWFnZS5yZXNpemVfaW1hZ2VzKGltZywgW3QsIHRdLCBtZXRob2Q9dGYuaW1hZ2UuUmVzaXplTWV0aG9kLkFSRUEsCiAgICAgICAgICAgICAgICBhbGlnbl9jb3JuZXJzPVRydWUpCiAgICBpbWcgICA9IHRmLmNhc3QodGYucmludCh0Zi5jbGlwX2J5X3ZhbHVlKGltZywgMCwgMjU1KSksIHRmLnVpbnQ4KQogICAgaW1nICAgPSB0Zi5pbWFnZS5yYW5kb21fY3JvcChpbWcsIDIgKiBbdGFyZ2V0X3Jlc10gKyBbY2hhbm5lbF9jb3VudF0pCiAgICByZXR1cm4gdGYuaW1hZ2UucmFuZG9tX2ZsaXBfbGVmdF9yaWdodChpbWcp" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">preprocess_image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">):</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">  </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">shape</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)[0],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">shape</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)[1]</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">minimum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random_crop</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[3])</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t_min</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">minimum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">round</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(9</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">))</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t_max</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">minimum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">round</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(12</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">))</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">     </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">uniform</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">([],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t_max</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">dtype</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">int32</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">resize_images</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">method</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">ResizeMethod</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">AREA</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">                </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">align_corners</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">True</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">cast</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">rint</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">clip_by_value</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">0,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">255)),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">uint8</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random_crop</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">channel_count</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">])</span>
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random_flip_left_right</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float">Listing 1: </span>TensorFlow <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">abadi2016tensorflow</span>)</cite> image preprocessing code for training dVAE. We use <span class="ltx_text ltx_font_typewriter">target_res = 256</span> and <span class="ltx_text ltx_font_typewriter">channel_count = 3</span>.</figcaption>
</figure>
<div id="A1.SS2.p1" class="ltx_para">
<p class="ltx_p">The dVAE is trained on the same dataset as the transformer, using the data augmentation code given in Listing <a href="#LST1" title="Listing 1 ‣ A.2 Training ‣ Appendix A Details for Discrete VAE ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Several quantities are decayed during training, all of which use a cosine schedule:</p>
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p class="ltx_p">The KL weight <math id="A1.I1.i1.p1.m1" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> is increased from <math id="A1.I1.i1.p1.m2" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math> to <math id="A1.I1.i1.p1.m3" class="ltx_Math" alttext="6.6" display="inline"><mn>6.6</mn></math> over the first <math id="A1.I1.i1.p1.m4" class="ltx_Math" alttext="5000" display="inline"><mn>5000</mn></math> updates. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bowman2015generating</span></cite> use a similar schedule based on the sigmoid function.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p class="ltx_p">The relaxation temperature <math id="A1.I1.i2.p1.m1" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> is annealed from <math id="A1.I1.i2.p1.m2" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> to <math id="A1.I1.i2.p1.m3" class="ltx_Math" alttext="1/16" display="inline"><mrow><mn>1</mn><mo>/</mo><mn>16</mn></mrow></math> over the first <math id="A1.I1.i2.p1.m4" class="ltx_Math" alttext="150000" display="inline"><mn>150000</mn></math> updates. Using a linear annealing schedule for this typically led to divergence.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p class="ltx_p">The step size is annealed from <math id="A1.I1.i3.p1.m1" class="ltx_Math" alttext="1\cdot 10^{-4}" display="inline"><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow></math> to <math id="A1.I1.i3.p1.m2" class="ltx_Math" alttext="1.25\cdot 10^{-6}" display="inline"><mrow><mn>1.25</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></math> over <math id="A1.I1.i3.p1.m3" class="ltx_Math" alttext="1200000" display="inline"><mn>1200000</mn></math> updates.</p>
</div>
</li>
</ol>
<p class="ltx_p">The decay schedules for the relaxation temperature and the step size are especially important for stability and successful optimization.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p class="ltx_p">We update the parameters using AdamW <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">loshchilov2017decoupled</span>)</cite> with <math id="A1.SS2.p2.m1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow></math>, <math id="A1.SS2.p2.m2" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow></math>, <math id="A1.SS2.p2.m3" class="ltx_Math" alttext="\epsilon=10^{-8}" display="inline"><mrow><mi>ϵ</mi><mo>=</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow></math>, and weight decay multiplier <math id="A1.SS2.p2.m4" class="ltx_Math" alttext="10^{-4}" display="inline"><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></math>. We use exponentially weighted iterate averaging for the parameters with decay coefficient <math id="A1.SS2.p2.m5" class="ltx_Math" alttext="0.999" display="inline"><mn>0.999</mn></math>. The reconstruction term in the ELB is a joint distribution over the <math id="A1.SS2.p2.m6" class="ltx_Math" alttext="256\times 256\times 3" display="inline"><mrow><mn>256</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>256</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></math> values for the image pixels, and the KL term is a joint distribution over the <math id="A1.SS2.p2.m7" class="ltx_Math" alttext="32\times 32" display="inline"><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow></math> positions in the spatial grid output by the encoder. We divide the overall loss by <math id="A1.SS2.p2.m8" class="ltx_Math" alttext="256\times 256\times 3" display="inline"><mrow><mn>256</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>256</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></math>, so that the weight of the KL term becomes <math id="A1.SS2.p2.m9" class="ltx_Math" alttext="\beta/192" display="inline"><mrow><mi>β</mi><mo>/</mo><mn>192</mn></mrow></math>, where <math id="A1.SS2.p2.m10" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> is the KL weight. The model is trained in mixed-precision using standard (i.e., global) loss scaling on <math id="A1.SS2.p2.m11" class="ltx_Math" alttext="64" display="inline"><mn>64</mn></math> 16 GB NVIDIA V100 GPUs, with a per-GPU batch size of <math id="A1.SS2.p2.m12" class="ltx_Math" alttext="8" display="inline"><mn>8</mn></math>, resulting in a total batch size of 512. It is trained for a total of <math id="A1.SS2.p2.m13" class="ltx_Math" alttext="3000000" display="inline"><mn>3000000</mn></math> updates.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>The Logit-Laplace Distribution</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p class="ltx_p">The <math id="A1.SS3.p1.m1" class="ltx_Math" alttext="\ell_{1}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>1</mn></msub></math> and <math id="A1.SS3.p1.m2" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></math> reconstruction objectives are commonly used when training VAEs. These objectives correspond to using Laplace and Gaussian distributions for <math id="A1.SS3.p1.m3" class="ltx_Math" alttext="\ln p_{\theta}(x\,|\,y,z)" display="inline"><mrow><mrow><mi>ln</mi><mo lspace="0.167em">⁡</mo><msub><mi>p</mi><mi>θ</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo fence="false" lspace="0.448em" rspace="0.448em">|</mo><mrow><mi>y</mi><mo>,</mo><mi>z</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math> in Equation <a href="#S2.E1" title="In 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, respectively. There is a strange mismatch in this modeling choice: pixel values lie within a bounded interval, but both of these distributions are supported by the entire real line. Hence, some amount of likelihood will be placed outside the admissible range of pixel values.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p class="ltx_p">We present a variant of the Laplace distribution that is also supported by a bounded interval. This resolves the discrepancy between the range of the pixel values being modeled and the support of the distribution used to model them. We consider the pdf of the random variable obtained by applying the sigmoid function to a Laplace-distributed random variable. This pdf is defined on <math id="A1.SS3.p2.m1" class="ltx_Math" alttext="(0,1)" display="inline"><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> and is given by</p>
<table id="A1.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.E2.m1" class="ltx_Math" alttext="f(x\,|\,\mu,b)=\frac{1}{2bx(1-x)}\exp\left(-\frac{|\operatorname{logit}(x)-\mu%
|}{b}\right);" display="block"><mrow><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo fence="false" lspace="0.448em" rspace="0.448em">|</mo><mrow><mi>μ</mi><mo>,</mo><mi>b</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mo>⁢</mo><mi>b</mi><mo>⁢</mo><mi>x</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo lspace="0.167em">⁢</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>−</mo><mfrac><mrow><mo stretchy="false">|</mo><mrow><mrow><mi>logit</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mi>μ</mi></mrow><mo stretchy="false">|</mo></mrow><mi>b</mi></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>;</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">we call it the <em class="ltx_emph ltx_font_italic">logit-Laplace distribution.</em> We use the logarithm of the RHS of Equation <a href="#A1.E2" title="In A.3 The Logit-Laplace Distribution ‣ Appendix A Details for Discrete VAE ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> as the reconstruction term for the training objective of the dVAE.</p>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<p class="ltx_p">The decoder of the dVAE produces six feature maps representing the sufficient statistics of the logit-Laplace distribution for the RGB channels of the image being reconstructed. The first three feature maps represent the <math id="A1.SS3.p3.m1" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math> parameter for the RGB channels, and the last three represent <math id="A1.SS3.p3.m2" class="ltx_Math" alttext="\ln b" display="inline"><mrow><mi>ln</mi><mo lspace="0.167em">⁡</mo><mi>b</mi></mrow></math>. Before feeding an image into the dVAE encoder, we transform its values using <math id="A1.SS3.p3.m3" class="ltx_Math" alttext="\varphi:[0,255]\to(\epsilon,1-\epsilon)" display="inline"><mrow><mi>φ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>255</mn><mo stretchy="false">]</mo></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, which is given by</p>
<table id="A1.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A1.E3.m1" class="ltx_Math" alttext="\varphi:x\mapsto\frac{1-2\epsilon}{255}x+\epsilon." display="block"><mrow><mrow><mi>φ</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>x</mi><mo stretchy="false">↦</mo><mrow><mrow><mfrac><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo>⁢</mo><mi>ϵ</mi></mrow></mrow><mn>255</mn></mfrac><mo>⁢</mo><mi>x</mi></mrow><mo>+</mo><mi>ϵ</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">This restricts the range of the pixel values to be modeled by the dVAE decoder to <math id="A1.SS3.p3.m4" class="ltx_Math" alttext="(\epsilon,1-\epsilon)" display="inline"><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><mo stretchy="false">)</mo></mrow></math>, which avoids numerical problems arising from the <math id="A1.SS3.p3.m5" class="ltx_Math" alttext="x(1-x)" display="inline"><mrow><mi>x</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> in Equation <a href="#A1.E2" title="In A.3 The Logit-Laplace Distribution ‣ Appendix A Details for Discrete VAE ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We use <math id="A1.SS3.p3.m6" class="ltx_Math" alttext="\epsilon=0.1" display="inline"><mrow><mi>ϵ</mi><mo>=</mo><mn>0.1</mn></mrow></math>. To reconstruct an image for manual inspection or computing metrics, we ignore <math id="A1.SS3.p3.m7" class="ltx_Math" alttext="\ln b" display="inline"><mrow><mi>ln</mi><mo lspace="0.167em">⁡</mo><mi>b</mi></mrow></math> and compute <math id="A1.SS3.p3.m8" class="ltx_Math" alttext="\hat{x}=\varphi^{-1}(\operatorname{sigmoid}(\mu))" display="inline"><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mrow><msup><mi>φ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>sigmoid</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>μ</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, where <math id="A1.SS3.p3.m9" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math> is given by the first three feature maps output by the dVAE decoder.<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>
              <span class="ltx_tag ltx_tag_note">11</span>
              
              
              
            See <span class="ltx_text ltx_font_typewriter">notebooks/usage.ipynb</span> of the code release for an example.</span></span></span></p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details for Transformer</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Architecture</h3>

<figure id="A2.F10" class="ltx_figure"><img src="xf_embds.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Illustration of the embedding scheme for a hypothetical version of our transformer with a maximum text length of 6 tokens. Each box denotes a vector of size <math id="A2.F10.m2" class="ltx_Math" alttext="d_{\mathrm{model}}=3968" display="inline"><mrow><msub><mi>d</mi><mi>model</mi></msub><mo>=</mo><mn>3968</mn></mrow></math>. In this illustration, the caption has a length of 4 tokens, so 2 padding tokens are used (as described in Section <a href="#S2.SS2" title="2.2 Stage Two: Learning the Prior ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>). Each image vocabulary embedding is summed with a row and column embedding.</figcaption>
</figure>
<figure id="A2.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="attn_row.png" id="A2.F11.sf1.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Row attention mask.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="attn_col.png" id="A2.F11.sf2.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Column attention mask.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F11.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="attn_col_t.png" id="A2.F11.sf3.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Column attention mask with transposed image states.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F11.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="attn_conv.png" id="A2.F11.sf4.g1" class="ltx_graphics ltx_img_square" width="144" height="144" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Convolutional attention mask.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Illustration of the three types of attention masks for a hypothetical version of our transformer with a maximum text length of 6 tokens and image length of 16 tokens (i.e., corresponding to a <math id="A2.F11.m4" class="ltx_Math" alttext="4\times 4" display="inline"><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>4</mn></mrow></math> grid). Mask (a) corresponds to row attention in which each image token attends to the previous 5 image tokens in raster order. The extent is chosen to be 5, so that the last token being attended to is the one in the same column of the previous row. To obtain better GPU utilization, we transpose the row and column dimensions of the image states when applying column attention, so that we can use mask (c) instead of mask (b). Mask (d) corresponds to a causal convolutional attention pattern with wraparound behavior (similar to the row attention) and a <math id="A2.F11.m5" class="ltx_Math" alttext="3\times 3" display="inline"><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></math> kernel. Our model uses a mask corresponding to an <math id="A2.F11.m6" class="ltx_Math" alttext="11\times 11" display="inline"><mrow><mn>11</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>11</mn></mrow></math> kernel.</figcaption>
</figure>
<div id="A2.SS1.p1" class="ltx_para">
<p class="ltx_p">Our model is a decoder-only sparse transformer of the same kind described in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">child2019generating</span></cite>, with broadcasted row and column embeddings for the part of the context for the image tokens. A complete description of the embedding scheme used in our model is shown in Figure <a href="#A2.F10" title="Figure 10 ‣ B.1 Architecture ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. We use 64 attention layers, each of which uses 62 attention heads with a per-head state size of 64.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p class="ltx_p">The model uses three kinds of sparse attention masks, which we show in Figure <a href="#A2.F11" title="Figure 11 ‣ B.1 Architecture ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. The convolutional attention mask (Figure <a href="#A2.F11" title="Figure 11 ‣ B.1 Architecture ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>(d)) is only used in the last self-attention layer. Otherwise, given the index <math id="A2.SS1.p2.m1" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> of a self-attention layer (with <math id="A2.SS1.p2.m2" class="ltx_Math" alttext="i\in[1,63]" display="inline"><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>63</mn><mo stretchy="false">]</mo></mrow></mrow></math>), we use the column attention mask (Figure <a href="#A2.F11" title="Figure 11 ‣ B.1 Architecture ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>(c)) if <math id="A2.SS1.p2.m3" class="ltx_Math" alttext="i-2\!\!\mod 4=0" display="inline"><mrow><mrow><mrow><mi>i</mi><mo>−</mo><mpadded width="0.448em"><mn>2</mn></mpadded></mrow><mo>mod</mo><mn>4</mn></mrow><mo>=</mo><mn>0</mn></mrow></math>, and row attention otherwise. E.g., the first four self-attention layers use “row, column, row, row”, respectively. With the exception of the convolutional attention mask, which we found to provide a small boost in performance over the row and dense causal attention masks when used in the final self-attention layer, this is the same configuration used in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">child2019generating</span></cite>.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Training</h3>

<figure id="LST2" class="ltx_float ltx_lstlisting">
<div class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,ZGVmIHByZXByb2Nlc3NfaW1hZ2UoaW1nLCB0YXJnZXRfcmVzKToKICAgIGgsIHcgID0gdGYuc2hhcGUoaW1nKVswXSwgdGYuc2hhcGUoaW1nKVsxXQogICAgc19taW4gPSB0Zi5taW5pbXVtKGgsIHcpCgogICAgb2ZmX2ggPSB0Zi5yYW5kb20udW5pZm9ybShbXSwgMyAqIChoIC0gc19taW4pIC8vIDgsCiAgICAgICAgdGYubWF4aW11bSgzICogKGggLSBzX21pbikgLy8gOCArIDEsIDUgKiAoaCAtIHNfbWluKSAvLyA4KSwKICAgICAgICBkdHlwZT10Zi5pbnQzMikKICAgIG9mZl93ID0gdGYucmFuZG9tLnVuaWZvcm0oW10sIDMgKiAodyAtIHNfbWluKSAvLyA4LAogICAgICAgIHRmLm1heGltdW0oMyAqICh3IC0gc19taW4pIC8vIDggKyAxLCA1ICogKHcgLSBzX21pbikgLy8gOCksCiAgICAgICAgZHR5cGU9dGYuaW50MzIpCgogICAgIyBSYW5kb20gZnVsbCBzcXVhcmUgY3JvcC4KICAgIGltZyAgID0gdGYuaW1hZ2UuY3JvcF90b19ib3VuZGluZ19ib3goaW1nLCBvZmZfaCwgb2ZmX3csIHNfbWluLCBzX21pbikKICAgIHRfbWF4ID0gdGYubWluaW11bShzX21pbiwgcm91bmQoOSAvIDggKiB0YXJnZXRfcmVzKSkKICAgIHQgICAgID0gdGYucmFuZG9tLnVuaWZvcm0oW10sIHRhcmdldF9yZXMsIHRfbWF4ICsgMSwgZHR5cGU9dGYuaW50MzIpCiAgICBpbWcgICA9IHRmLmltYWdlLnJlc2l6ZV9pbWFnZXMoaW1nLCBbdCwgdF0sIG1ldGhvZD10Zi5pbWFnZS5SZXNpemVNZXRob2QuQVJFQSwKICAgICAgICAgICAgICAgIGFsaWduX2Nvcm5lcnM9VHJ1ZSkKICAgIGltZyAgID0gdGYuY2FzdCh0Zi5yaW50KHRmLmNsaXBfYnlfdmFsdWUoaW1nLCAwLCAyNTUpKSwgdGYudWludDgpCgogICAgIyBXZSBkb24ndCB1c2UgaGZsaXAgYXVnIHNpbmNlIHRoZSBpbWFnZSBtYXkgY29udGFpbiB0ZXh0LgogICAgcmV0dXJuIHRmLmltYWdlLnJhbmRvbV9jcm9wKGltZywgMiAqIFt0YXJnZXRfcmVzXSArIFtjaGFubmVsX2NvdW50XSk=" download="">⬇</a></div>
<div id="lstnumberx14" class="ltx_listingline">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">preprocess_image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">):</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">  </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">shape</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)[0],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">shape</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)[1]</span>
</div>
<div id="lstnumberx16" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">minimum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx17" class="ltx_listingline">
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">off_h</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">uniform</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">([],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">3</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">//</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8,</span>
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">        </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">maximum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(3</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">//</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">5</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">h</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">//</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8),</span>
</div>
<div id="lstnumberx20" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">        </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">dtype</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">int32</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx21" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">off_w</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">uniform</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">([],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">3</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">//</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8,</span>
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">        </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">maximum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(3</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">//</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">5</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">w</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">//</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8),</span>
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">        </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">dtype</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">int32</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
</div>
<div id="lstnumberx25" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">#</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">Random</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">full</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">square</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">crop</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span>
</div>
<div id="lstnumberx26" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">crop_to_bounding_box</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">off_h</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">off_w</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx27" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t_max</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">minimum</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">s_min</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">round</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(9</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">))</span>
</div>
<div id="lstnumberx28" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">     </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">uniform</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">([],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t_max</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">dtype</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">int32</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx29" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">resize_images</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">method</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">ResizeMethod</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">AREA</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span>
</div>
<div id="lstnumberx30" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">                </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">align_corners</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">True</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx31" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">   </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">cast</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">rint</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">clip_by_value</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">0,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">255)),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">uint8</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">)</span>
</div>
<div id="lstnumberx32" class="ltx_listingline">
</div>
<div id="lstnumberx33" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">#</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">We</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">don</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">t</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">use</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">hflip</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">aug</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">since</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">may</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">contain</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">text</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span>
</div>
<div id="lstnumberx34" class="ltx_listingline">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;">    </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">tf</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">image</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">random_crop</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">img</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">target_res</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:80%;">channel_count</span><span class="ltx_text ltx_font_typewriter" style="font-size:80%;">])</span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float">Listing 2: </span>TensorFlow <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">abadi2016tensorflow</span>)</cite> image preprocessing code for training the transformer. We use <span class="ltx_text ltx_font_typewriter">target_res = 256</span> and <span class="ltx_text ltx_font_typewriter">channel_count = 3</span>.</figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p class="ltx_p">When training the transformer, we apply data augmentation to the images before encoding them using the dVAE encoder. We use slightly different augmentations from the ones used to train the dVAE; the code used for this is given in Listing <a href="#LST2" title="Listing 2 ‣ B.2 Training ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We also apply 10% BPE dropout when BPE-encoding the captions for training. The model is trained using per-resblock scaling (see Section <a href="#S2.SS4" title="2.4 Mixed-Precision Training ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>) and gradient compression (see Section <a href="#S2.SS5" title="2.5 Distributed Optimization ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>) with total compression rank 896 (so that each GPU uses a compression rank of 112 for its parameter shards). As shown in Table <a href="#S2.T1" title="Table 1 ‣ 2.5 Distributed Optimization ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this results in a compression rate of about 86%, which we analyze in Section <a href="#A5.SS1" title="E.1 Bandwidth Analysis ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E.1</span></a>.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p class="ltx_p">We update the parameters using AdamW with <math id="A2.SS2.p2.m1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow></math>, <math id="A2.SS2.p2.m2" class="ltx_Math" alttext="\beta_{2}=0.96" display="inline"><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.96</mn></mrow></math>, <math id="A2.SS2.p2.m3" class="ltx_Math" alttext="\epsilon=10^{-8}" display="inline"><mrow><mi>ϵ</mi><mo>=</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow></math>, and weight decay multiplier <math id="A2.SS2.p2.m4" class="ltx_Math" alttext="4.5\cdot 10^{-2}" display="inline"><mrow><mn>4.5</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow></math>. We clip the decompressed gradients by norm using a threshold of 4, prior to applying the Adam update. Gradient clipping is only triggered during the warm-up phase at the start of training. To conserve memory, most Adam moments (see Section <a href="#A4" title="Appendix D Guidelines for Mixed-Precision Training ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> for details) are stored in 16-bit formats, with a 1-6-9 format for the running mean (i.e., 1 bit for the sign, 6 bits for the exponent, and 9 bits for the significand), and a 0-6-10 format for the running variance. We clip the estimate for running variance by value to 5 before it is used to update the parameters or moments. Finally, we apply exponentially weighted iterate averaging by asynchronously copying the model parameters from the GPU to the CPU once every 25 updates, using a decay coefficient of 0.99.</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p class="ltx_p">We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of <math id="A2.SS2.p3.m1" class="ltx_Math" alttext="1024" display="inline"><mn>1024</mn></math>, for a total of <math id="A2.SS2.p3.m2" class="ltx_Math" alttext="430000" display="inline"><mn>430000</mn></math> updates. At the start of training, we use a linear schedule to ramp up the step size to <math id="A2.SS2.p3.m3" class="ltx_Math" alttext="4.5\cdot 10^{-4}" display="inline"><mrow><mn>4.5</mn><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow></math> over <math id="A2.SS2.p3.m4" class="ltx_Math" alttext="5000" display="inline"><mn>5000</mn></math> updates, and halved the step size each time the training loss appeared to plateau. We did this a total of five times, ending training with a final step size that was 32 times smaller than the initial one. We reserved about <math id="A2.SS2.p3.m5" class="ltx_Math" alttext="606000" display="inline"><mn>606000</mn></math> images for validation, and did not observe overfitting at any point during training.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Details for Data Collection</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">In order to train the 12-billion parameter transformer, we created a dataset of a similar scale to JFT-300M by collecting 250 million text-image pairs from the internet. As described in Section <a href="#S2.SS3" title="2.3 Data Collection ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, this dataset incorporates Conceptual Captions, the text-image pairs from Wikipedia, and a filtered subset of YFCC100M. We use a subset of the text, image, and joint text and image filters described in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sharma2018conceptual</span></cite> to construct this dataset. These filters include discarding instances whose captions are too short, are classified as non-English by the Python package <span class="ltx_text ltx_font_typewriter">cld3</span>, or that consist primarily of boilerplate phrases such as “photographed on <span class="ltx_text ltx_font_typewriter">&lt;date&gt;</span>”, where <span class="ltx_text ltx_font_typewriter">&lt;date&gt;</span> matches various formats for dates that we found in the data. We also discard instances whose images have aspect ratios not in <math id="A3.p1.m1" class="ltx_Math" alttext="[1/2,2]" display="inline"><mrow><mo stretchy="false">[</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><mo>,</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></math>. If we were to use to very tall or wide images, then the square crops used during training would likely exclude objects mentioned in the caption.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Guidelines for Mixed-Precision Training</h2>

<figure id="A4.F12" class="ltx_figure"><img src="grad_scale_plot.png" id="A4.F12.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="240" height="321" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Plot of per-resblock gradient scales for a 2.8-billion parameter text-to-image transformer trained without gradient compression. The <math id="A4.F12.m4" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math>-axis is parameter updates, and the <math id="A4.F12.m5" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>-axis is the base-2 logarithm of the gradient scale. Darkest violet corresponds to the first resblock, and brightest yellow corresponds to the last (of which there are 128 total). The gradient scale for the second MLP resblock hovers at around <math id="A4.F12.m6" class="ltx_Math" alttext="2^{24}" display="inline"><msup><mn>2</mn><mn>24</mn></msup></math>, while the others stay within a 4-bit range. The extent of this range increases as the model is made larger.</figcaption>
</figure>
<div id="A4.p1" class="ltx_para">
<p class="ltx_p">The most challenging part of this project was getting the model to train in 16-bit precision past one billion parameters. We were able to do this after detecting for underflow in various parts of training, and revising the code to eliminate it. We developed a set of guidelines as a result of this process that we present here.<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup>
            <span class="ltx_tag ltx_tag_note">12</span>
            
            
            
          Fewer of these guidelines may be necessary on hardware like the TPU that has native support for the bfloat16 format, since the larger 8-bit exponent range makes underflow less likely to occur.</span></span></span></p>
<ol id="A4.I1" class="ltx_enumerate">
<li id="A4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A4.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Use per-resblock gradient scaling (Figure <a href="#S2.F4" title="Figure 4 ‣ 2.2 Stage Two: Learning the Prior ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) instead of standard loss scaling.</span> Our model uses 128 gradient scales, one for each of its resblocks. All of the gradient scales are initialized to <math id="A4.I1.i1.p1.m1" class="ltx_Math" alttext="M\cdot 2^{13}" display="inline"><mrow><mi>M</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>2</mn><mn>13</mn></msup></mrow></math>, where <math id="A4.I1.i1.p1.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> is the number of data-parallel replicas (i.e., the number of GPUs). In our setup, each grad scale is multiplied by <math id="A4.I1.i1.p1.m3" class="ltx_Math" alttext="2^{1/1000}" display="inline"><msup><mn>2</mn><mrow><mn>1</mn><mo>/</mo><mn>1000</mn></mrow></msup></math> at every parameter update when there are no nonfinite values for any parameter gradient in that resblock. Otherwise, we divide the grad scale by <math id="A4.I1.i1.p1.m4" class="ltx_Math" alttext="\sqrt{2}" display="inline"><msqrt><mn>2</mn></msqrt></math> and skip the update. We also disallow consecutive divisions of the same grad scale within a window of <math id="A4.I1.i1.p1.m5" class="ltx_Math" alttext="125" display="inline"><mn>125</mn></math> updates. All grad scales are clamped to the range <math id="A4.I1.i1.p1.m6" class="ltx_Math" alttext="[M\cdot 2^{7},M\cdot 2^{24}]" display="inline"><mrow><mo stretchy="false">[</mo><mrow><mi>M</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>2</mn><mn>7</mn></msup></mrow><mo>,</mo><mrow><mi>M</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msup><mn>2</mn><mn>24</mn></msup></mrow><mo stretchy="false">]</mo></mrow></math> after being updated. Figure <a href="#A4.F12" title="Figure 12 ‣ Appendix D Guidelines for Mixed-Precision Training ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows the gradient scales in the early phase of training for a 2.8-billion parameter model.</p>
</div>
</li>
<li id="A4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A4.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Only use 16-bit precision where it is really necessary for performance.</span> In particular, store all gains, biases, embeddings, and unembeddings in 32-bit precision, with 32-bit gradients (including for remote communication) and 32-bit Adam moments. We disable gradient compression for these parameters (though PowerSGD would not make sense for 1D parameters like gains and biases). The logits for the text and image tokens are computed and stored in 32-bit precision. We found that storing the embeddings in 16-bit precision sometimes caused divergence early in optimization, and using 16-bit logits resulted in a small shift in the training curve, so we switched to use 32-bit precision out of an abundance of caution.</p>
</div>
</li>
<li id="A4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A4.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Avoid underflow when dividing the gradient.</span> For data-parallel training, we need to divide the gradients by the total number of data-parallel workers <math id="A4.I1.i3.p1.m1" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math>. One way to do this is to divide the loss by the per-machine batch size, and then divide the parameter gradients by <math id="A4.I1.i3.p1.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> before summing them over the machines (using all-reduce). To save time and space, the gradients are usually computed and stored in 16-bit precision. When <math id="A4.I1.i3.p1.m3" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> is large, this division could result in underflow before the gradients are summed. On the other hand, if we attempt to sum the gradients first and then divide them later, we could encounter overflow in the all-reduce.</p>
</div>
<div id="A4.I1.i3.p2" class="ltx_para">
<p class="ltx_p">Our solution for this problem attempts to minimize the loss of information in the division prior to the all-reduce, without danger of overflow. To do this, we divide the loss by the overall batch size (which includes <math id="A4.I1.i3.p2.m1" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> as a factor) rather than the per-machine batch size, and multiply the gradient scales by <math id="A4.I1.i3.p2.m2" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> to compensate, as described in (1). Then, prior to the all-reduce operation, we divide the gradients by a constant that was tuned by hand to avoid both underflow and overflow. This was done by inspecting histograms of the exponents (i.e., base-2 logarithms) of the absolute values of the scalar components of the per-parameter gradients. Since the gradient scaling keeps the gradients close to right end of the exponent range of the 16-bit format, we found that the same constant worked well for all parameters in the model with 16-bit gradients. When using PowerSGD, we chose different constants for the <math id="A4.I1.i3.p2.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A4.I1.i3.p2.m4" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Details for Distributed Optimization</h2>

<div id="A5.p1" class="ltx_para">
<p class="ltx_p">We use PowerSGD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">vogels2019powersgd</span>)</cite> to compress the gradients with respect to all parameters except the embeddings, unembeddings, gains, and biases. In Section <a href="#A5.SS1" title="E.1 Bandwidth Analysis ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E.1</span></a>, we derive an expression for the reduction in the amount of data communicated as a function of the compression rank and model size. In Section <a href="#A5.SS2" title="E.2 Implementation Details ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E.2</span></a>, we present a detailed overview of our adaptation of PowerSGD, and the modifications we had to make in order to fix performance regressions, some of which only manifest at billion-parameter scale.</p>
</div>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Bandwidth Analysis</h3>

<figure id="A5.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Parameter Names</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text" style="font-size:90%;">Parameter Shard Gradient Shape (No Compression)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="A5.T2.m1" class="ltx_Math" alttext="P" display="inline"><mi mathsize="90%">P</mi></math><span class="ltx_text" style="font-size:90%;"> shape</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="A5.T2.m2" class="ltx_Math" alttext="Q" display="inline"><mi mathsize="90%">Q</mi></math><span class="ltx_text" style="font-size:90%;"> shape</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_text" style="font-size:90%;">qkv and post-attention matrices</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="A5.T2.m3" class="ltx_Math" alttext="d\times(d/m)" display="inline"><mrow><mi mathsize="90%">d</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">d</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="A5.T2.m4" class="ltx_Math" alttext="d\times(r/m)" display="inline"><mrow><mi mathsize="90%">d</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math id="A5.T2.m5" class="ltx_Math" alttext="(r/m)\times(d/m)" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em">)</mo></mrow><mo mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">d</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_text" style="font-size:90%;">First MLP matrix</span></th>
<td class="ltx_td ltx_align_center"><math id="A5.T2.m6" class="ltx_Math" alttext="d\times(4d/m)" display="inline"><mrow><mi mathsize="90%">d</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mrow><mn mathsize="90%">4</mn><mo>⁢</mo><mi mathsize="90%">d</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math id="A5.T2.m7" class="ltx_Math" alttext="d\times(r/m)" display="inline"><mrow><mi mathsize="90%">d</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math id="A5.T2.m8" class="ltx_Math" alttext="(r/m)\times(4d/m)" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em">)</mo></mrow><mo mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mrow><mn mathsize="90%">4</mn><mo>⁢</mo><mi mathsize="90%">d</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_text" style="font-size:90%;">Second MLP matrix</span></th>
<td class="ltx_td ltx_align_center"><math id="A5.T2.m9" class="ltx_Math" alttext="(4d/m)\times d" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mrow><mn mathsize="90%">4</mn><mo>⁢</mo><mi mathsize="90%">d</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em">)</mo></mrow><mo mathsize="90%" rspace="0.222em">×</mo><mi mathsize="90%">d</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math id="A5.T2.m10" class="ltx_Math" alttext="(4d/m)\times(r/m)" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mrow><mn mathsize="90%">4</mn><mo>⁢</mo><mi mathsize="90%">d</mi></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em">)</mo></mrow><mo mathsize="90%" rspace="0.222em">×</mo><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math id="A5.T2.m11" class="ltx_Math" alttext="(r/m)\times d" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mi mathsize="90%">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow><mo maxsize="90%" minsize="90%" rspace="0.055em">)</mo></mrow><mo mathsize="90%" rspace="0.222em">×</mo><mi mathsize="90%">d</mi></mrow></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text" style="font-size:90%;">Total size</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb"><math id="A5.T2.m12" class="ltx_Math" alttext="12d^{2}/m" display="inline"><mrow><mrow><mn mathsize="90%">12</mn><mo>⁢</mo><msup><mi mathsize="90%">d</mi><mn mathsize="90%">2</mn></msup></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><mi mathsize="90%">m</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math id="A5.T2.m13" class="ltx_Math" alttext="(5drm+4dr)/m^{2}" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mrow><mn mathsize="90%">5</mn><mo>⁢</mo><mi mathsize="90%">d</mi><mo>⁢</mo><mi mathsize="90%">r</mi><mo>⁢</mo><mi mathsize="90%">m</mi></mrow><mo mathsize="90%">+</mo><mrow><mn mathsize="90%">4</mn><mo>⁢</mo><mi mathsize="90%">d</mi><mo>⁢</mo><mi mathsize="90%">r</mi></mrow></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><msup><mi mathsize="90%">m</mi><mn mathsize="90%">2</mn></msup></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math id="A5.T2.m14" class="ltx_Math" alttext="(drm+8dr)/m^{2}" display="inline"><mrow><mrow><mo maxsize="90%" minsize="90%">(</mo><mrow><mrow><mi mathsize="90%">d</mi><mo>⁢</mo><mi mathsize="90%">r</mi><mo>⁢</mo><mi mathsize="90%">m</mi></mrow><mo mathsize="90%">+</mo><mrow><mn mathsize="90%">8</mn><mo>⁢</mo><mi mathsize="90%">d</mi><mo>⁢</mo><mi mathsize="90%">r</mi></mrow></mrow><mo maxsize="90%" minsize="90%">)</mo></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true">/</mo><msup><mi mathsize="90%">m</mi><mn mathsize="90%">2</mn></msup></mrow></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>We analyze the amount of data sent from each GPU on a given machine to GPUs on other machines, in the case where we shard the parameters among the <math id="A5.T2.m21" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> GPUs on each machine. Here, <math id="A5.T2.m22" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math> denotes the rank used for compression, and <math id="A5.T2.m23" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> the transformer hidden size. The compression ratio is given by the sum of the last two columns of the last row, divided by the first column of the last row. This comes out to <math id="A5.T2.m24" class="ltx_Math" alttext="r(m+2)/(2dm)" display="inline"><mrow><mrow><mi>r</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>m</mi><mo>+</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>m</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, which for <math id="A5.T2.m25" class="ltx_Math" alttext="m=8" display="inline"><mrow><mi>m</mi><mo>=</mo><mn>8</mn></mrow></math> is <math id="A5.T2.m26" class="ltx_Math" alttext="5r/8d" display="inline"><mrow><mrow><mrow><mn>5</mn><mo>⁢</mo><mi>r</mi></mrow><mo>/</mo><mn>8</mn></mrow><mo>⁢</mo><mi>d</mi></mrow></math>.</figcaption>
</figure>
<div id="A5.SS1.p1" class="ltx_para">
<p class="ltx_p">Gradient compression uses the factorization <math id="A5.SS1.p1.m1" class="ltx_Math" alttext="G\approx PQ^{t}" display="inline"><mrow><mi>G</mi><mo>≈</mo><mrow><mi>P</mi><mo>⁢</mo><msup><mi>Q</mi><mi>t</mi></msup></mrow></mrow></math>, where <math id="A5.SS1.p1.m2" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.SS1.p1.m3" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> both have rank <math id="A5.SS1.p1.m4" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math>. Instead of using a single all-reduce to transmit <math id="A5.SS1.p1.m5" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math>, we use two, smaller all-reduces to transmit both <math id="A5.SS1.p1.m6" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.SS1.p1.m7" class="ltx_Math" alttext="Q^{t}" display="inline"><msup><mi>Q</mi><mi>t</mi></msup></math> in succession. Hence, the compression ratio is the sum of the sizes of the <math id="A5.SS1.p1.m8" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.SS1.p1.m9" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices divided by the sum of the sizes of the <math id="A5.SS1.p1.m10" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> matrices. We shard along axis 1 for all parameters except for the second MLP matrix. The derivation of the compression ratio in our setup is given in Table <a href="#A5.T2" title="Table 2 ‣ E.1 Bandwidth Analysis ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We note that the choice of shard axis changes the compression ratio for the MLP matrices. Finally, this analysis excludes the embeddings, unembeddings, gains, and biases, for which we do not use compression. The total fraction of the bandwidth used by these parameters becomes smaller as the model size is increased.</p>
</div>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Implementation Details</h3>

<div id="A5.SS2.p1" class="ltx_para">
<p class="ltx_p">We describe the steps in our implementation of PowerSGD in detail, since these details were crucial in getting it to work efficiently and reliably at billion-parameter scale.</p>
<ol id="A5.I1" class="ltx_enumerate">
<li id="A5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A5.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Our training setup uses a combination of parameter sharding and gradient compression, as described in Section <a href="#S2.SS5" title="2.5 Distributed Optimization ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>. During backpropagation, while recomputing the activations and computing the gradients for the current resblock, we prefetch the parameters for the preceding resblock using all-gather. Once each GPU has computed the gradient with respect to a full parameter matrix, we compute the average of the slice of the gradient corresponding to the GPU’s parameter shard, and discard the full gradient immediately to conserve memory. This average is taken over all of the GPUs on a machine using reduce-scatter.</p>
</div>
</li>
<li id="A5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A5.I1.i2.p1" class="ltx_para">
<p class="ltx_p">If there are no nonfinite values in the result of the reduce-scatter (which could be caused by overflow in backpropagation or the reduce-scatter), we divide the result by the resblock’s gradient scale, and add it to the error buffer (i.e., the buffer used for error correction). Otherwise, we do nothing and proceed with backpropagation; a single nonfinite value in the gradient means that the entire update will be skipped, which happens about 5% of the time. The error buffer uses the same 1-6-9 format used for the Adam mean, which we describe in Section <a href="#A2.SS2" title="B.2 Training ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>; the larger exponent range ensures that this division does not result in underflow. Adding the gradients directly to the error buffers avoids redundantly allocating another set of buffers of size equal to the parameter shard gradients.</p>
</div>
</li>
<li id="A5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A5.I1.i3.p1" class="ltx_para">
<p class="ltx_p">Once the reduce-scatter operations for the resblock have finished, we schedule the operations to compute the <math id="A5.I1.i3.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices from the errors buffers and the <math id="A5.I1.i3.p1.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices, whose values are fixed at the start of training (see Section <a href="#S2.SS5" title="2.5 Distributed Optimization ‣ 2 Method ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>). Both the <math id="A5.I1.i3.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I1.i3.p1.m4" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices are stored in 1-6-9 format and have their values scaled by predetermined constants, as discussed in Section <a href="#A4" title="Appendix D Guidelines for Mixed-Precision Training ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</li>
<li id="A5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A5.I1.i4.p1" class="ltx_para">
<p class="ltx_p">Once each GPU has computed the <math id="A5.I1.i4.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices for the parameter shards in a resblock, they are averaged with the <math id="A5.I1.i4.p1.m2" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices from the GPUs with the same ordinal on all other machines, using a single, grouped all-reduce operation. This all-reduce is carried out in the 1-6-9 format, using a custom kernel. The grouping results in better bandwidth utilization, since it avoids scheduling many all-reduce calls for smaller, individual parameters, each of which carries some overhead. We clamp any infinities in the results of the all-reduce to the maximum value of the 1-6-9 format (which is slightly less than 16), retaining the sign. With our choice of scaling factors for the <math id="A5.I1.i4.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I1.i4.p1.m4" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices, this clamping happens very rarely.</p>
</div>
</li>
<li id="A5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A5.I1.i5.p1" class="ltx_para">
<p class="ltx_p">Once the all-reduce operation for the <math id="A5.I1.i5.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices for a resblock have finished, we orthogonalize the columns of the resulting matrices. We use a custom Householder orthogonalization kernel rather than Gram-Schmidt, as we found the latter to be numerically unstable. We also add <math id="A5.I1.i5.p1.m2" class="ltx_Math" alttext="\epsilon I_{m\times r}" display="inline"><mrow><mi>ϵ</mi><mo>⁢</mo><msub><mi>I</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>r</mi></mrow></msub></mrow></math> to <math id="A5.I1.i5.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> in order to ensure that the result is not near rank-deficient, where <math id="A5.I1.i5.p1.m4" class="ltx_Math" alttext="\epsilon=10^{-6}" display="inline"><mrow><mi>ϵ</mi><mo>=</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow></math>. Here, <math id="A5.I1.i5.p1.m5" class="ltx_Math" alttext="I_{m\times r}" display="inline"><msub><mi>I</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>r</mi></mrow></msub></math> is a rectangular matrix of the same size as the <math id="A5.I1.i5.p1.m6" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrix to which it is added; it contains the <math id="A5.I1.i5.p1.m7" class="ltx_Math" alttext="r\times r" display="inline"><mrow><mi>r</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>r</mi></mrow></math> identity matrix and has zeros elsewhere. The orthogonalizalied <math id="A5.I1.i5.p1.m8" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices are stored in 1-6-9 format, but without scaling.</p>
</div>
</li>
<li id="A5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A5.I1.i6.p1" class="ltx_para">
<p class="ltx_p">Once the <math id="A5.I1.i6.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices for a resblock have been orthogonalized, we schedule the operations to compute the new <math id="A5.I1.i6.p1.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices from the error buffers and the <math id="A5.I1.i6.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices.</p>
</div>
</li>
<li id="A5.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A5.I1.i7.p1" class="ltx_para">
<p class="ltx_p">Once the new <math id="A5.I1.i7.p1.m1" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices for a resblock have been computed, we schedule another grouped all-reduce, similar to what we did for the <math id="A5.I1.i7.p1.m2" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrices. As in step (4), we clamp all infinities in the results of the all-reduce to the maximum value of the 1-6-9 format, retaining the sign. The error buffers for the resblock have now been decomposed into low-rank factors <math id="A5.I1.i7.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I1.i7.p1.m4" class="ltx_Math" alttext="Q^{t}" display="inline"><msup><mi>Q</mi><mi>t</mi></msup></math>.</p>
</div>
</li>
<li id="A5.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A5.I1.i8.p1" class="ltx_para">
<p class="ltx_p">The gradients for all parameters that are not compressed are grouped together into a single, 32-bit precision all-reduce. Section <a href="#A4" title="Appendix D Guidelines for Mixed-Precision Training ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> explains why we use 32-bit precision for these parameters and their gradients.</p>
</div>
</li>
<li id="A5.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A5.I1.i9.p1" class="ltx_para">
<p class="ltx_p">Once all GPUs on a machine have finished steps (7) and (8) for every resblock in the model, the values of the <math id="A5.I1.i9.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I1.i9.p1.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices for the same parameter shard on all machines will be identical. We then compute the global gradient norm, which is the sum of two quantities: (a) the sum of the squared Frobenius norms of the <math id="A5.I1.i9.p1.m3" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices over all of the parameter shards on a machine, and (b) the sum of the squared norms of the gradients for the parameter shards that do not use compression, taken over all such parameter shards on a machine. We need to compute this value for gradient clipping (see Section <a href="#A2.SS2" title="B.2 Training ‣ Appendix B Details for Transformer ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>).</p>
</div>
</li>
<li id="A5.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A5.I1.i10.p1" class="ltx_para">
<p class="ltx_p">While computing the global norm, we also synchronize the information from step (2) about which parameter shard gradients contained nonfinite values after the reduce-scatter. After doing this, we have two pieces of information for each parameter shard: (a) whether its error buffer from step (2) contains nonfinite values on the current GPU, and (b) whether <math id="A5.I1.i10.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> or <math id="A5.I1.i10.p1.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> contains nonfinite values. We cannot rely on the values of the <math id="A5.I1.i10.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I1.i10.p1.m4" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices to determine (b), since we clamp infinities as described in step (4). If we find that the gradient with respect to any parameter shard on the machine contains nonfinite values, then we set the global norm to infinity.</p>
</div>
</li>
<li id="A5.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">11.</span> 
<div id="A5.I1.i11.p1" class="ltx_para">
<p class="ltx_p">Once all of the all-reduces have finished and the global norm has been computed, we can apply the parameter updates. Like backpropagation, the parameter updates proceed resblock-by-resblock. The first step is to compute the decompressed gradients by forming the product <math id="A5.I1.i11.p1.m1" class="ltx_Math" alttext="PQ^{t}" display="inline"><mrow><mi>P</mi><mo>⁢</mo><msup><mi>Q</mi><mi>t</mi></msup></mrow></math> for all parameters in a given resblock. To avoid overflow, these products are computed in 32-bit precision. We can then apply the Adam update to the parameters using the decompressed gradients and the global norm computed in step (9). If the global norm is not finite, then the update to the parameters and Adam moments is skipped. We note that the decompressed gradient must be divided by the scale of the <math id="A5.I1.i11.p1.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrix (the <math id="A5.I1.i11.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> matrix is stored without scaling after orthogonalization).</p>
</div>
</li>
<li id="A5.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">12.</span> 
<div id="A5.I1.i12.p1" class="ltx_para">
<p class="ltx_p">The second step is the update to the error buffers. First, we use the results from step (10) to check if the <math id="A5.I1.i12.p1.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I1.i12.p1.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> matrices for a given parameter shard contain only finite values. If this is the case, then we divide the decompressed gradient by the total number of machines, and subtract it from the current value for the error buffer. This sets the error buffer to the difference between the “local” gradient averaged over the GPUs on the machine using reduce-scatter, and the “remote” decompressed gradient (i.e., the “error”). If either <math id="A5.I1.i12.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> or <math id="A5.I1.i12.p1.m4" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> contains nonfinite values, then we check if the error buffer computed in step (2) contains only finite values. If it does, then we preserve its value and do nothing. If it does not, then we set it to zero. The purpose of this tedious logic is to set an error buffer to zero only when we must do so, because it has been contaminated with nonfinite values. We found that error buffers getting set to zero too frequently by gradient scaling events leads to performance regressions.</p>
</div>
</li>
<li id="A5.I1.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">13.</span> 
<div id="A5.I1.i13.p1" class="ltx_para">
<p class="ltx_p">The parameter shards whose gradients are not compressed are updated separately.</p>
</div>
</li>
</ol>
</div>
<div id="A5.SS2.p2" class="ltx_para">
<p class="ltx_p">We also note the following important optimizations:</p>
<ol id="A5.I2" class="ltx_enumerate">
<li id="A5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A5.I2.i1.p1" class="ltx_para">
<p class="ltx_p">There are several opportunities for overlap between compute and communication in the above steps. For example, while we are running step (2) for resblock <math id="A5.I2.i1.p1.m1" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>, we can proceed to steps (3)–(8) for all resblocks <math id="A5.I2.i1.p1.m2" class="ltx_Math" alttext="j&gt;i" display="inline"><mrow><mi>j</mi><mo>&gt;</mo><mi>i</mi></mrow></math>. Exploiting opportunities for overlap is necessary to achieve good performance.</p>
</div>
</li>
<li id="A5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A5.I2.i2.p1" class="ltx_para">
<p class="ltx_p">We throttle specific operations that are liable to exhaust all available memory. For example, we only prefetch the parameters from the preceding resblock when the reduce-scatter operations have finished for the current one. Otherwise, we risk running out of memory by holding on to the full parameters. We also throttle the Adam updates, so that we do not decompress all of the gradients at once.</p>
</div>
</li>
<li id="A5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A5.I2.i3.p1" class="ltx_para">
<p class="ltx_p">There are two places in the implementation where the transposition matters: (a) the choice of shard axis for the MLP matrices and (b) whether we compute the low-rank factorization for a gradient or its transpose. The former influences the bandwidth analysis, which we present in Section <a href="#A5.SS1" title="E.1 Bandwidth Analysis ‣ Appendix E Details for Distributed Optimization ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E.1</span></a>. The latter influences the cost of the orthogonalization. Suppose that the gradient <math id="A5.I2.i3.p1.m1" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> is <math id="A5.I2.i3.p1.m2" class="ltx_Math" alttext="m\times n" display="inline"><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></math> and its low-rank factors <math id="A5.I2.i3.p1.m3" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> and <math id="A5.I2.i3.p1.m4" class="ltx_Math" alttext="Q^{t}" display="inline"><msup><mi>Q</mi><mi>t</mi></msup></math> are <math id="A5.I2.i3.p1.m5" class="ltx_Math" alttext="m\times r" display="inline"><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>r</mi></mrow></math> and <math id="A5.I2.i3.p1.m6" class="ltx_Math" alttext="r\times n" display="inline"><mrow><mi>r</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>n</mi></mrow></math>, respectively, with <math id="A5.I2.i3.p1.m7" class="ltx_Math" alttext="r\ll m,n" display="inline"><mrow><mi>r</mi><mo>≪</mo><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></mrow></math>. To make orthogonalization cheaper, we transpose <math id="A5.I2.i3.p1.m8" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> appropriately so that <math id="A5.I2.i3.p1.m9" class="ltx_Math" alttext="m\leqslant n" display="inline"><mrow><mi>m</mi><mo>⩽</mo><mi>n</mi></mrow></math>.</p>
</div>
<div id="A5.I2.i3.p2" class="ltx_para">
<p class="ltx_p">At first glance, it may seem like a limitation that the NCCL all-gather and reduce-scatter primitives shard along axis 0 only. We may need to transpose some matrices before and after communication operations because of (a) and (b), which would require additional time and potentially special care to avoid out-of-memory errors. In fact, we never actually needed to do this. This is because we stored some of the parameters in their transposed formats and exploited the <span class="ltx_text ltx_font_typewriter">transpose_a</span> and <span class="ltx_text ltx_font_typewriter">transpose_b</span> parameters of the matrix multiplication kernels used in forward propagation, backpropagation, and steps (1)–(13) above. This allowed us to avoid explicit transposition while retaining the freedom to choose how to handle (a) and (b).</p>
</div>
</li>
<li id="A5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A5.I2.i4.p1" class="ltx_para">
<p class="ltx_p">In step (12) above, we note that setting the error buffers to zero too often can cause performance regressions. We wanted to avoid doing this when resuming training from a checkpoint, which happens more frequently for larger jobs as it is likely that a machine will periodically fail. Naively, this would require uploading the error buffers from all of the machines along with the model checkpoints. Since we use a total of 128 machines for training, this would lead to 128 times greater storage usage, which is extremely wasteful.</p>
</div>
<div id="A5.I2.i4.p2" class="ltx_para">
<p class="ltx_p">Fortunately, this is unnecessary, as error correction depends only on the sum of the error buffers. This property follows from linearity and the sequence of operations used by PowerSGD. Hence, it suffices to store the sums of the errors buffers taken across all GPUs with the same ordinal. When resuming from a checkpoint, we can divide the error buffers by the total number of machines and broadcast them.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Details for Human Evaluation Experiments</h2>

<figure id="A6.F13" class="ltx_figure"><img src="example_human_evals_task.png" id="A6.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="334" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Example task interface shown to workers.</figcaption>
</figure>
<div id="A6.p1" class="ltx_para">
<p class="ltx_p">We start with a list of <math id="A6.p1.m1" class="ltx_Math" alttext="1000" display="inline"><mn>1000</mn></math> captions and generate one sample image per model per caption. Captions and sample images are then used to create <math id="A6.p1.m2" class="ltx_Math" alttext="1000" display="inline"><mn>1000</mn></math> image comparison tasks per experiment, which we submitted to Amazon’s Mechanical Turk. Each task was answered by five distinct workers. Workers were asked to compare two images and answer two questions about them: (1) which image is most realistic, and (2) which image best matches the shared caption. The experimental setup provided to workers is shown in Figure <a href="#A6.F13" title="Figure 13 ‣ Appendix F Details for Human Evaluation Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. One worker’s answers were disqualified due to a high rate of disagreement with other workers combined with a fast answer velocity (with many submission times under 4 seconds); all other worker answers were kept.</p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Zero-Shot Image-to-Image Translation</h2>

<figure id="A7.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A7.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sketch_0.png" id="A7.F14.sf1.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sketch_1.png" id="A7.F14.sf1.g2" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sketch_2.png" id="A7.F14.sf1.g3" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sketch_3.png" id="A7.F14.sf1.g4" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>“the exact same cat on the top as a sketch on the bottom”</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A7.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="reflection_0.png" id="A7.F14.sf2.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="reflection_1.png" id="A7.F14.sf2.g2" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="reflection_2.png" id="A7.F14.sf2.g3" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="reflection_3.png" id="A7.F14.sf2.g4" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>“the exact same photo on the top reflected upside-down on the bottom”</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A7.F14.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="close_up_0.png" id="A7.F14.sf3.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="close_up_1.png" id="A7.F14.sf3.g2" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="close_up_2.png" id="A7.F14.sf3.g3" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="close_up_3.png" id="A7.F14.sf3.g4" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>“2 panel image of the exact same cat. on the top, a photo of the cat. on the bottom, an extreme close-up view of the cat in the photo.”</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A7.F14.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="red_0.png" id="A7.F14.sf4.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="red_1.png" id="A7.F14.sf4.g2" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="red_2.png" id="A7.F14.sf4.g3" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="red_3.png" id="A7.F14.sf4.g4" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>“the exact same cat on the top colored red on the bottom”</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A7.F14.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sunglasses_0.png" id="A7.F14.sf5.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sunglasses_1.png" id="A7.F14.sf5.g2" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sunglasses_2.png" id="A7.F14.sf5.g3" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="sunglasses_3.png" id="A7.F14.sf5.g4" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>“2 panel image of the exact same cat. on the top, a photo of the cat. on the bottom, the cat with sunglasses.”</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A7.F14.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="postage_stamp_0.png" id="A7.F14.sf6.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="postage_stamp_1.png" id="A7.F14.sf6.g2" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="postage_stamp_2.png" id="A7.F14.sf6.g3" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;padding:0.0pt;"><img src="postage_stamp_3.png" id="A7.F14.sf6.g4" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>“the exact same cat on the top as a postage stamp on the bottom”</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Further examples of zero-shot image-to-image translation.</figcaption>
</figure>
<div id="A7.p1" class="ltx_para">
<p class="ltx_p">Figure <a href="#A7.F14" title="Figure 14 ‣ Appendix G Zero-Shot Image-to-Image Translation ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> shows further examples of zero-shot image-to-image translation, which we discussed in Section <a href="#S3.SS3" title="3.3 Qualitative Findings ‣ 3 Experiments ‣ Zero-Shot Text-to-Image Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. We did not anticipate that this capability would emerge, and made no modifications to the training procedure to encourage it.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jul  4 15:05:41 2025 by <a href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>
</body>
</html>
